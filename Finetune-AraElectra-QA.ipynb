{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1172cfc2",
   "metadata": {},
   "source": [
    "## Fine-tune AraElectra on Custom Dataset\n",
    "\n",
    "This is a POC for Fine-tuning AraElectra or other available pre-trained models: https://huggingface.co/aubmindlab on a Custom Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b4dd748",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.finetuningQA.data_split import train_dev_test_split, combine_json_files\n",
    "import json\n",
    "import datasets\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a475d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AraElectra model local path. \n",
    "# You can download another pretrained models from here: https://huggingface.co/aubmindlab\n",
    "model_path= \"araelectra-base-discriminator/\"\n",
    "model_name = \"aubmindlab/araelectra-base-discriminator\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677458f8",
   "metadata": {},
   "source": [
    "## Combine multiple datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c716005",
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine_json_files([\"dataset/question-answering-dataset/Arabic-SQuAD.json\",\"dataset/question-answering-dataset/arcd-train.json\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e337eeac",
   "metadata": {},
   "source": [
    "## Splitt the dataset into train/dev/test (if needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66b3ce5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_dev_test_split(\"dataset/question-answering-dataset/Arabic-SQuAD.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d1e5d0",
   "metadata": {},
   "source": [
    "## Pre-process the data\n",
    "Here we will prepare the dataset in the format that the model expect.\n",
    "\n",
    "The Dataset structure must be as follow:\n",
    "\n",
    "'''\n",
    "\n",
    "    {\"data\": \n",
    "\n",
    "        [ \n",
    "            {\"paragraphs\": \n",
    "\n",
    "                [\n",
    "                    {\"qas\": \n",
    "            \n",
    "                    [{\"question\": \"ما هو برنامج مزايا\", \"id\": 445985, \n",
    "\n",
    "                        \"answers\": [\"text\": \"هو أحد المبادرات الجديدة التي تطرحها وزارة الإسكان والتخطيط العمراني بالتعاون مع القطاع الخاص لتوفير السكن الاجتماعي للمواطنين المدرجة أسمائهم على قوائم الانتظار\"}, \n",
    "            \n",
    "                    {\"question\": \"ما هو اقصى حد لقيمة القسط الشهري المترتب على المواطن؟\", \"id\": 445986, \n",
    "                \n",
    "                        \"answers\": [\"text\": \"25% من راتب المواطن كحد أقصى.\"]\n",
    "\n",
    "                    \"context\": \"نبذة عن برنامج \"مزايا \" برنامج \"مزايا \" هو أحد المبادرات الجديدة التي تطرحها وزارة الإسكان والتخطيط العمراني بالتعاون مع القطاع الخاص لتوفير السكن الاجتماعي للمواطنين المدرجة أسمائهم على قوائم الانتظار، وتقوم فكرة البرنامج على قيام المواطن الذي تنطبق عليه المعايير بشراء وحدة سكنية من خلال حصوله على تمويل من أحد البنوك المشاركة، على أن تقوم الحكومة بتوفير الدعم المالي للمواطن، والمتمثل في سداد الفارق بين قيمة القسط الفعلي لمبلغ التمويل المحدد من قبل البنك الممول، وقيمة القسط الشهري المستحق على المواطن والذي لا يتجاوز 25% من راتب المواطن كحد أقصى. \",\n",
    "            \"document_id\": 862498\"}\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "\n",
    "    }\n",
    "\n",
    "        \n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5e14ef2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "aubmindlab/araelectra-base-discriminator\n",
      " 72%|█████████████████████████████▋           | 232/321 [00:15<00:03, 26.47it/s]WARNING:tensorflow:Could not find answer for question '555832005630' :\n",
      " 'الصين ( بالصينية المبسطة : تشونغوا ؛ بالصينية التقليدية : ) المعروفة رسميا باسم جمهورية الصين الشعبية ( بالصينية المبسطة : تشونغهوا رنمين غونغهيغو ؛ بالصينية التقليدية : ) هي الدولة الأكثر سكانا في العالم حيث يقطنها أكثر من 1 . 338 مليار نسمة .' \n",
      "vs.\n",
      " '中国'\n",
      "orig answer:\n",
      " '中国'\n",
      "==================\n",
      "WARNING:tensorflow:Could not find answer for question '930272774853' :\n",
      " 'الصين ( بالصينية المبسطة : تشونغوا ؛ بالصينية التقليدية : ) المعروفة رسميا باسم جمهورية الصين الشعبية ( بالصينية المبسطة : تشونغهوا رنمين غونغهيغو ؛ بالصينية التقليدية : ) هي الدولة الأكثر سكانا في العالم حيث يقطنها أكثر من 1 . 338 مليار نسمة .' \n",
      "vs.\n",
      " '中华人民共和国'\n",
      "orig answer:\n",
      " '中华人民共和国'\n",
      "==================\n",
      "100%|█████████████████████████████████████████| 321/321 [00:15<00:00, 20.72it/s]\n",
      "WARNING:tensorflow:Found 0 new answers: \n",
      "WARNING:tensorflow:Found 2 with no answers: \n",
      "WARNING:tensorflow:Found 0 with trunc answers: \n",
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "aubmindlab/araelectra-base-discriminator\n",
      "100%|██████████████████████████████████████████| 78/78 [00:00<00:00, 260.02it/s]\n",
      "WARNING:tensorflow:Found 0 new answers: \n",
      "WARNING:tensorflow:Found 0 with no answers: \n",
      "WARNING:tensorflow:Found 0 with trunc answers: \n"
     ]
    }
   ],
   "source": [
    "# Training Dataset\n",
    "!python src/finetuningQA/preprocessing.py \\\n",
    "  --input_file=\"dataset/question-answering-dataset/turk_combined_all.json\" \\\n",
    "  --output_file=\"dataset/question-answering-dataset/turk_combined_all_preprocessed.json\" \\\n",
    "  --model_name=$model_name\n",
    "\n",
    "\n",
    "# Eval Dataset\n",
    "\n",
    "!python src/finetuningQA/preprocessing.py \\\n",
    "  --input_file=\"dataset/question-answering-dataset/arcd-test.json\" \\\n",
    "  --output_file=\"dataset/question-answering-dataset/arcd-test_preprocessed.json\" \\\n",
    "  --model_name=$model_name\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d3ab47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the preprocessed dataset and print one sample to see how the preprocessed data looks like\n",
    "with open(\"dataset/question-answering-dataset/turk_combined_all_preprocessed.json\") as f:\n",
    " data = json.load(f)['data']\n",
    "\n",
    "print(data[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4155cce",
   "metadata": {},
   "source": [
    "## Custom Dataset Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "240d2b6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/finetuningQA/custom_dataset_loader.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/finetuningQA/custom_dataset_loader.py\n",
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import json\n",
    "import logging\n",
    "\n",
    "import datasets\n",
    "\n",
    "\n",
    "_CITATION = \"\"\"\\\n",
    "@inproceedings{mozannar-etal-2019-neural,\n",
    "    title = {Neural {A}rabic Question Answering},\n",
    "    author = {Mozannar, Hussein  and Maamary, Elie  and El Hajal, Karl  and Hajj, Hazem},\n",
    "    booktitle = {Proceedings of the Fourth Arabic Natural Language Processing Workshop},\n",
    "    month = {aug},\n",
    "    year = {2019},\n",
    "    address = {Florence, Italy},\n",
    "    publisher = {Association for Computational Linguistics},\n",
    "    url = {https://www.aclweb.org/anthology/W19-4612},\n",
    "    doi = {10.18653/v1/W19-4612},\n",
    "    pages = {108--118},\n",
    "    abstract = {This paper tackles the problem of open domain factual Arabic question answering (QA) using Wikipedia as our knowledge source. This constrains the answer of any question to be a span of text in Wikipedia. Open domain QA for Arabic entails three challenges: annotated QA datasets in Arabic, large scale efficient information retrieval and machine reading comprehension. To deal with the lack of Arabic QA datasets we present the Arabic Reading Comprehension Dataset (ARCD) composed of 1,395 questions posed by crowdworkers on Wikipedia articles, and a machine translation of the Stanford Question Answering Dataset (Arabic-SQuAD). Our system for open domain question answering in Arabic (SOQAL) is based on two components: (1) a document retriever using a hierarchical TF-IDF approach and (2) a neural reading comprehension model using the pre-trained bi-directional transformer BERT. Our experiments on ARCD indicate the effectiveness of our approach with our BERT-based reader achieving a 61.3 F1 score, and our open domain system SOQAL achieving a 27.6 F1 score.}\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "_DESCRIPTION = \"\"\"\\\n",
    " Custom Dataset Loader \n",
    "\"\"\"\n",
    "\n",
    "_URL = \"../../dataset/question-answering-dataset/\"\n",
    "_URLs = {\n",
    "    \"train\": _URL + \"turk_combined_all_preprocessed.json\",\n",
    "    \"dev\": _URL + \"arcd-test_preprocessed.json\",\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "class CustomDatasetAraElectraConfig(datasets.BuilderConfig):\n",
    "    \"\"\"BuilderConfig for Custom Dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        \"\"\"BuilderConfig for CustomDataset.\n",
    "\n",
    "        Args:\n",
    "          **kwargs: keyword arguments forwarded to super.\n",
    "        \"\"\"\n",
    "        super(CustomDatasetAraElectraConfig, self).__init__(**kwargs)\n",
    "\n",
    "\n",
    "class CustomDatasetAraElectra(datasets.GeneratorBasedBuilder):\n",
    "    \"\"\"CustomDataset\"\"\"\n",
    "\n",
    "    BUILDER_CONFIGS = [\n",
    "        CustomDatasetAraElectraConfig(\n",
    "            name=\"plain_text\",\n",
    "            version=datasets.Version(\"1.0.0\", \"\"),\n",
    "            description=\"Plain text\",\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    def _info(self):\n",
    "        return datasets.DatasetInfo(\n",
    "            description=_DESCRIPTION,\n",
    "            features=datasets.Features(\n",
    "                {\n",
    "                    \"id\": datasets.Value(\"string\"),\n",
    "                    \"title\": datasets.Value(\"string\"),\n",
    "                    \"context\": datasets.Value(\"string\"),\n",
    "                    \"question\": datasets.Value(\"string\"),\n",
    "                    \"answers\": datasets.features.Sequence(\n",
    "                        {\"text\": datasets.Value(\"string\"), \"answer_start\": datasets.Value(\"int32\")}\n",
    "                    ),\n",
    "                }\n",
    "            ),\n",
    "            # No default supervised_keys (as we have to pass both question\n",
    "            # and context as input).\n",
    "            supervised_keys=None,\n",
    "            citation=_CITATION,\n",
    "        )\n",
    "\n",
    "    def _split_generators(self, dl_manager):\n",
    "        urls_to_download = _URLs\n",
    "        downloaded_files = dl_manager.download_and_extract(urls_to_download)\n",
    "\n",
    "        return [\n",
    "            datasets.SplitGenerator(name=datasets.Split.TRAIN, gen_kwargs={\"filepath\": downloaded_files[\"train\"]}),\n",
    "            datasets.SplitGenerator(name=datasets.Split.VALIDATION, gen_kwargs={\"filepath\": downloaded_files[\"dev\"]}),\n",
    "        ]\n",
    "\n",
    "    def _generate_examples(self, filepath):\n",
    "        \"\"\"This function returns the examples in the raw (text) form.\"\"\"\n",
    "        logging.info(\"generating examples from = %s\", filepath)\n",
    "        with open(filepath, encoding=\"utf-8\") as f:\n",
    "            data_ = json.load(f)\n",
    "            for article in data_[\"data\"]:\n",
    "                title = article.get(\"title\", \"\").strip()\n",
    "                for paragraph in article[\"paragraphs\"]:\n",
    "                    context = paragraph[\"context\"].strip()\n",
    "                    for qa in paragraph[\"qas\"]:\n",
    "                        question = qa[\"question\"].strip()\n",
    "\n",
    "                        id_ = qa[\"id\"]\n",
    "\n",
    "                        answer_starts = [answer[\"answer_start\"] for answer in qa[\"answers\"]]\n",
    "                        answers = [answer[\"text\"].strip() for answer in qa[\"answers\"]]                     \n",
    "\n",
    "                        # Features currently used are \"context\", \"question\", and \"answers\".\n",
    "                        # Others are extracted here for the ease of future expansions.\n",
    "                        yield id_, {\n",
    "                            \"title\": title,\n",
    "                            \"context\": context,\n",
    "                            \"question\": question,\n",
    "                            \"id\": id_,\n",
    "                            \"answers\": {\"answer_start\": answer_starts, \"text\": answers},\n",
    "                        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "99bd1d0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset custom_dataset_loader/plain_text to /home/mohammed/.cache/huggingface/datasets/custom_dataset_loader/plain_text/1.0.0/3d570e1d36dad6068475d5aab1fcc90b73dd204edf020b0f330a206c94a2bc29...\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.013733863830566406,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading data files",
       "rate": null,
       "total": 2,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbf4f975b8254f60a12489f175da1dc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.04159402847290039,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Extracting data files",
       "rate": null,
       "total": 2,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6b47db2cbe74850ba2938e8c8ac0e1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.014218568801879883,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Generating train split",
       "rate": null,
       "total": 0,
       "unit": " examples",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c387c2fe30b24beead4fdf508e73d726",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.012883901596069336,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Generating validation split",
       "rate": null,
       "total": 0,
       "unit": " examples",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b7386064da64c29b377a33eb6446783",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset custom_dataset_loader downloaded and prepared to /home/mohammed/.cache/huggingface/datasets/custom_dataset_loader/plain_text/1.0.0/3d570e1d36dad6068475d5aab1fcc90b73dd204edf020b0f330a206c94a2bc29. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.015983104705810547,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 2,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7abcf5485012444aa45d988b320aa0d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
      "        num_rows: 49065\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
      "        num_rows: 702\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "squad = datasets.load_dataset(\"src/finetuningQA/custom_dataset_loader.py\")\n",
    "print(squad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "512064d7",
   "metadata": {},
   "source": [
    "## Fine-tuning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9c181511",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/09/2022 17:16:43 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "10/09/2022 17:16:43 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=epoch,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=3e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=-1,\n",
      "log_level_replica=-1,\n",
      "log_on_each_node=True,\n",
      "logging_dir=./run/runs/Oct09_17-16-43_mohammed-GL75-Leopard-10SFR,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=500,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=5.0,\n",
      "optim=adamw_hf,\n",
      "output_dir=./run,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=8,\n",
      "per_device_train_batch_size=8,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=['mlflow'],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=./run,\n",
      "save_on_each_node=False,\n",
      "save_steps=10000,\n",
      "save_strategy=steps,\n",
      "save_total_limit=None,\n",
      "seed=666,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tf32=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=500,\n",
      "weight_decay=0.0,\n",
      "xpu_backend=None,\n",
      ")\n",
      "10/09/2022 17:16:43 - WARNING - datasets.builder -   Found cached dataset custom_dataset_loader (/home/mohammed/.cache/huggingface/datasets/custom_dataset_loader/plain_text/1.0.0/3d570e1d36dad6068475d5aab1fcc90b73dd204edf020b0f330a206c94a2bc29)\n",
      "100%|███████████████████████████████████████████| 2/2 [00:00<00:00, 1030.41it/s]\n",
      "[INFO|configuration_utils.py:679] 2022-10-09 17:16:43,339 >> loading configuration file araelectra-base-discriminator/config.json\n",
      "[INFO|configuration_utils.py:730] 2022-10-09 17:16:43,347 >> Model config ElectraConfig {\n",
      "  \"_name_or_path\": \"araelectra-base-discriminator/\",\n",
      "  \"architectures\": [\n",
      "    \"ElectraForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_size\": 768,\n",
      "  \"generator_hidden_size\": 0.33333,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.21.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 64000\n",
      "}\n",
      "\n",
      "[INFO|configuration_utils.py:679] 2022-10-09 17:16:43,348 >> loading configuration file araelectra-base-discriminator/config.json\n",
      "[INFO|configuration_utils.py:730] 2022-10-09 17:16:43,348 >> Model config ElectraConfig {\n",
      "  \"_name_or_path\": \"araelectra-base-discriminator/\",\n",
      "  \"architectures\": [\n",
      "    \"ElectraForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_size\": 768,\n",
      "  \"generator_hidden_size\": 0.33333,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.21.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 64000\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1723] 2022-10-09 17:16:43,349 >> Didn't find file araelectra-base-discriminator/added_tokens.json. We won't load it.\n",
      "[INFO|tokenization_utils_base.py:1801] 2022-10-09 17:16:43,349 >> loading file araelectra-base-discriminator/vocab.txt\n",
      "[INFO|tokenization_utils_base.py:1801] 2022-10-09 17:16:43,349 >> loading file araelectra-base-discriminator/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:1801] 2022-10-09 17:16:43,349 >> loading file None\n",
      "[INFO|tokenization_utils_base.py:1801] 2022-10-09 17:16:43,349 >> loading file araelectra-base-discriminator/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:1801] 2022-10-09 17:16:43,349 >> loading file araelectra-base-discriminator/tokenizer_config.json\n",
      "[INFO|configuration_utils.py:679] 2022-10-09 17:16:43,349 >> loading configuration file araelectra-base-discriminator/config.json\n",
      "[INFO|configuration_utils.py:730] 2022-10-09 17:16:43,350 >> Model config ElectraConfig {\n",
      "  \"_name_or_path\": \"araelectra-base-discriminator/\",\n",
      "  \"architectures\": [\n",
      "    \"ElectraForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_size\": 768,\n",
      "  \"generator_hidden_size\": 0.33333,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.21.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 64000\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:2039] 2022-10-09 17:16:43,479 >> loading weights file araelectra-base-discriminator/pytorch_model.bin\n",
      "[WARNING|modeling_utils.py:2426] 2022-10-09 17:16:44,459 >> Some weights of the model checkpoint at araelectra-base-discriminator/ were not used when initializing ElectraForQuestionAnswering: ['discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense_prediction.weight']\n",
      "- This IS expected if you are initializing ElectraForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "[WARNING|modeling_utils.py:2438] 2022-10-09 17:16:44,459 >> Some weights of ElectraForQuestionAnswering were not initialized from the model checkpoint at araelectra-base-discriminator/ and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|███████████████████████████████████████████| 50/50 [00:11<00:00,  4.41ba/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00,  4.60ba/s]\n",
      "src/finetuningQA/run_qa.py:489: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric(\"squad_v2\" if data_args.version_2_with_negative else \"squad\")\n",
      "[INFO|trainer.py:1891] 2022-10-09 17:17:12,208 >> Loading model from araelectra-base-discriminator/.\n",
      "[WARNING|trainer.py:2008] 2022-10-09 17:17:12,550 >> There were missing keys in the checkpoint model loaded: ['qa_outputs.weight', 'qa_outputs.bias'].\n",
      "[WARNING|trainer.py:2011] 2022-10-09 17:17:12,550 >> There were unexpected keys in the checkpoint model loaded: ['discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense_prediction.bias'].\n",
      "/home/mohammed/anaconda3/envs/python37/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "[INFO|trainer.py:1605] 2022-10-09 17:17:12,564 >> ***** Running training *****\n",
      "[INFO|trainer.py:1606] 2022-10-09 17:17:12,564 >>   Num examples = 49357\n",
      "[INFO|trainer.py:1607] 2022-10-09 17:17:12,564 >>   Num Epochs = 5\n",
      "[INFO|trainer.py:1608] 2022-10-09 17:17:12,564 >>   Instantaneous batch size per device = 8\n",
      "[INFO|trainer.py:1609] 2022-10-09 17:17:12,564 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "[INFO|trainer.py:1610] 2022-10-09 17:17:12,564 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1611] 2022-10-09 17:17:12,564 >>   Total optimization steps = 30850\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "  0%|                                                 | 0/30850 [00:00<?, ?it/s][INFO|trainer.py:2063] 2022-10-09 17:17:12,854 >> Didn't find an RNG file, if you are resuming a training that was launched in a distributed fashion, reproducibility is not guaranteed.\n",
      "{'loss': 4.103, 'learning_rate': 3e-05, 'epoch': 0.08}                          \n",
      "{'loss': 2.6577, 'learning_rate': 2.9505766062602963e-05, 'epoch': 0.16}        \n",
      "{'loss': 2.5057, 'learning_rate': 2.9011532125205933e-05, 'epoch': 0.24}        \n",
      "{'loss': 2.3886, 'learning_rate': 2.8517298187808895e-05, 'epoch': 0.32}        \n",
      "{'loss': 2.2995, 'learning_rate': 2.802306425041186e-05, 'epoch': 0.41}         \n",
      "{'loss': 2.2831, 'learning_rate': 2.7528830313014827e-05, 'epoch': 0.49}        \n",
      "{'loss': 2.2163, 'learning_rate': 2.7034596375617793e-05, 'epoch': 0.57}        \n",
      "{'loss': 2.1618, 'learning_rate': 2.654036243822076e-05, 'epoch': 0.65}         \n",
      "{'loss': 2.2174, 'learning_rate': 2.6046128500823725e-05, 'epoch': 0.73}        \n",
      "{'loss': 2.2015, 'learning_rate': 2.5551894563426688e-05, 'epoch': 0.81}        \n",
      "{'loss': 2.1275, 'learning_rate': 2.5057660626029657e-05, 'epoch': 0.89}        \n",
      "{'loss': 2.1164, 'learning_rate': 2.456342668863262e-05, 'epoch': 0.97}         \n",
      " 20%|███████▏                            | 6170/30850 [48:31<2:56:05,  2.34it/s][INFO|trainer.py:723] 2022-10-09 18:05:44,437 >> The following columns in the evaluation set don't have a corresponding argument in `ElectraForQuestionAnswering.forward` and have been ignored: offset_mapping, example_id. If offset_mapping, example_id are not expected by `ElectraForQuestionAnswering.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:3452] 2022-10-09 18:05:44,438 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:3453] 2022-10-09 18:05:44,438 >>   Num examples = 708\n",
      "[INFO|trainer.py:3454] 2022-10-09 18:05:44,438 >>   Batch size = 8\n",
      "/home/mohammed/anaconda3/envs/python37/lib/python3.7/site-packages/transformers/trainer_pt_utils.py:394: FutureWarning: DistributedTensorGatherer is deprecated and will be removed in v5 of Transformers.\n",
      "  FutureWarning,\n",
      "\n",
      "  0%|                                                    | 0/89 [00:00<?, ?it/s]\u001b[A\n",
      "  2%|▉                                           | 2/89 [00:00<00:06, 14.46it/s]\u001b[A\n",
      "  4%|█▉                                          | 4/89 [00:00<00:09,  9.07it/s]\u001b[A\n",
      "  7%|██▉                                         | 6/89 [00:00<00:10,  8.08it/s]\u001b[A\n",
      "  8%|███▍                                        | 7/89 [00:00<00:10,  7.82it/s]\u001b[A\n",
      "  9%|███▉                                        | 8/89 [00:00<00:10,  7.62it/s]\u001b[A\n",
      " 10%|████▍                                       | 9/89 [00:01<00:10,  7.47it/s]\u001b[A\n",
      " 11%|████▊                                      | 10/89 [00:01<00:10,  7.37it/s]\u001b[A\n",
      " 12%|█████▎                                     | 11/89 [00:01<00:10,  7.30it/s]\u001b[A\n",
      " 13%|█████▊                                     | 12/89 [00:01<00:10,  7.23it/s]\u001b[A\n",
      " 15%|██████▎                                    | 13/89 [00:01<00:10,  7.12it/s]\u001b[A\n",
      " 16%|██████▊                                    | 14/89 [00:01<00:10,  7.04it/s]\u001b[A\n",
      " 17%|███████▏                                   | 15/89 [00:01<00:10,  7.00it/s]\u001b[A\n",
      " 18%|███████▋                                   | 16/89 [00:02<00:10,  6.97it/s]\u001b[A\n",
      " 19%|████████▏                                  | 17/89 [00:02<00:10,  6.94it/s]\u001b[A\n",
      " 20%|████████▋                                  | 18/89 [00:02<00:10,  6.94it/s]\u001b[A\n",
      " 21%|█████████▏                                 | 19/89 [00:02<00:10,  6.92it/s]\u001b[A\n",
      " 22%|█████████▋                                 | 20/89 [00:02<00:09,  6.92it/s]\u001b[A\n",
      " 24%|██████████▏                                | 21/89 [00:02<00:09,  6.95it/s]\u001b[A\n",
      " 25%|██████████▋                                | 22/89 [00:02<00:09,  6.96it/s]\u001b[A\n",
      " 26%|███████████                                | 23/89 [00:03<00:09,  6.94it/s]\u001b[A\n",
      " 27%|███████████▌                               | 24/89 [00:03<00:09,  6.93it/s]\u001b[A\n",
      " 28%|████████████                               | 25/89 [00:03<00:09,  6.92it/s]\u001b[A\n",
      " 29%|████████████▌                              | 26/89 [00:03<00:09,  6.92it/s]\u001b[A\n",
      " 30%|█████████████                              | 27/89 [00:03<00:08,  6.96it/s]\u001b[A\n",
      " 31%|█████████████▌                             | 28/89 [00:03<00:08,  6.98it/s]\u001b[A\n",
      " 33%|██████████████                             | 29/89 [00:03<00:08,  6.99it/s]\u001b[A\n",
      " 34%|██████████████▍                            | 30/89 [00:04<00:08,  7.01it/s]\u001b[A\n",
      " 35%|██████████████▉                            | 31/89 [00:04<00:08,  7.02it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 32/89 [00:04<00:08,  7.02it/s]\u001b[A\n",
      " 37%|███████████████▉                           | 33/89 [00:04<00:07,  7.02it/s]\u001b[A\n",
      " 38%|████████████████▍                          | 34/89 [00:04<00:07,  7.03it/s]\u001b[A\n",
      " 39%|████████████████▉                          | 35/89 [00:04<00:07,  7.02it/s]\u001b[A\n",
      " 40%|█████████████████▍                         | 36/89 [00:04<00:07,  6.97it/s]\u001b[A\n",
      " 42%|█████████████████▉                         | 37/89 [00:05<00:07,  6.94it/s]\u001b[A\n",
      " 43%|██████████████████▎                        | 38/89 [00:05<00:07,  6.93it/s]\u001b[A\n",
      " 44%|██████████████████▊                        | 39/89 [00:05<00:07,  6.91it/s]\u001b[A\n",
      " 45%|███████████████████▎                       | 40/89 [00:05<00:07,  6.92it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 41/89 [00:05<00:06,  6.91it/s]\u001b[A\n",
      " 47%|████████████████████▎                      | 42/89 [00:05<00:06,  6.91it/s]\u001b[A\n",
      " 48%|████████████████████▊                      | 43/89 [00:06<00:06,  6.91it/s]\u001b[A\n",
      " 49%|█████████████████████▎                     | 44/89 [00:06<00:06,  6.89it/s]\u001b[A\n",
      " 51%|█████████████████████▋                     | 45/89 [00:06<00:06,  6.89it/s]\u001b[A\n",
      " 52%|██████████████████████▏                    | 46/89 [00:06<00:06,  6.91it/s]\u001b[A\n",
      " 53%|██████████████████████▋                    | 47/89 [00:06<00:06,  6.90it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 48/89 [00:06<00:05,  6.91it/s]\u001b[A\n",
      " 55%|███████████████████████▋                   | 49/89 [00:06<00:05,  6.90it/s]\u001b[A\n",
      " 56%|████████████████████████▏                  | 50/89 [00:07<00:05,  6.89it/s]\u001b[A\n",
      " 57%|████████████████████████▋                  | 51/89 [00:07<00:05,  6.89it/s]\u001b[A\n",
      " 58%|█████████████████████████                  | 52/89 [00:07<00:05,  6.89it/s]\u001b[A\n",
      " 60%|█████████████████████████▌                 | 53/89 [00:07<00:05,  6.90it/s]\u001b[A\n",
      " 61%|██████████████████████████                 | 54/89 [00:07<00:05,  6.95it/s]\u001b[A\n",
      " 62%|██████████████████████████▌                | 55/89 [00:07<00:04,  6.95it/s]\u001b[A\n",
      " 63%|███████████████████████████                | 56/89 [00:07<00:04,  6.92it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 57/89 [00:08<00:04,  6.90it/s]\u001b[A\n",
      " 65%|████████████████████████████               | 58/89 [00:08<00:04,  6.90it/s]\u001b[A\n",
      " 66%|████████████████████████████▌              | 59/89 [00:08<00:04,  6.89it/s]\u001b[A\n",
      " 67%|████████████████████████████▉              | 60/89 [00:08<00:04,  6.88it/s]\u001b[A\n",
      " 69%|█████████████████████████████▍             | 61/89 [00:08<00:04,  6.87it/s]\u001b[A\n",
      " 70%|█████████████████████████████▉             | 62/89 [00:08<00:03,  6.89it/s]\u001b[A\n",
      " 71%|██████████████████████████████▍            | 63/89 [00:08<00:03,  6.89it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 64/89 [00:09<00:03,  6.91it/s]\u001b[A\n",
      " 73%|███████████████████████████████▍           | 65/89 [00:09<00:03,  6.90it/s]\u001b[A\n",
      " 74%|███████████████████████████████▉           | 66/89 [00:09<00:03,  6.90it/s]\u001b[A\n",
      " 75%|████████████████████████████████▎          | 67/89 [00:09<00:03,  6.91it/s]\u001b[A\n",
      " 76%|████████████████████████████████▊          | 68/89 [00:09<00:03,  6.89it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▎         | 69/89 [00:09<00:02,  6.90it/s]\u001b[A\n",
      " 79%|█████████████████████████████████▊         | 70/89 [00:09<00:02,  6.91it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▎        | 71/89 [00:10<00:02,  6.90it/s]\u001b[A\n",
      " 81%|██████████████████████████████████▊        | 72/89 [00:10<00:02,  6.89it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 73/89 [00:10<00:02,  6.89it/s]\u001b[A\n",
      " 83%|███████████████████████████████████▊       | 74/89 [00:10<00:02,  6.88it/s]\u001b[A\n",
      " 84%|████████████████████████████████████▏      | 75/89 [00:10<00:02,  6.89it/s]\u001b[A\n",
      " 85%|████████████████████████████████████▋      | 76/89 [00:10<00:01,  6.89it/s]\u001b[A\n",
      " 87%|█████████████████████████████████████▏     | 77/89 [00:10<00:01,  6.88it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▋     | 78/89 [00:11<00:01,  6.89it/s]\u001b[A\n",
      " 89%|██████████████████████████████████████▏    | 79/89 [00:11<00:01,  6.90it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▋    | 80/89 [00:11<00:01,  6.94it/s]\u001b[A\n",
      " 91%|███████████████████████████████████████▏   | 81/89 [00:11<00:01,  6.95it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 82/89 [00:11<00:01,  6.99it/s]\u001b[A\n",
      " 93%|████████████████████████████████████████   | 83/89 [00:11<00:00,  7.00it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▌  | 84/89 [00:11<00:00,  7.01it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████  | 85/89 [00:12<00:00,  7.01it/s]\u001b[A\n",
      " 97%|█████████████████████████████████████████▌ | 86/89 [00:12<00:00,  7.02it/s]\u001b[A\n",
      " 98%|██████████████████████████████████████████ | 87/89 [00:12<00:00,  7.03it/s]\u001b[A\n",
      " 99%|██████████████████████████████████████████▌| 88/89 [00:12<00:00,  7.03it/s]\u001b[A\n",
      "100%|███████████████████████████████████████████| 89/89 [00:12<00:00,  7.01it/s]\u001b[A10/09/2022 18:05:57 - INFO - utils_qa -   Post-processing 702 example predictions split into 708 features.\n",
      "\n",
      "\n",
      "  0%|                                                   | 0/702 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "  7%|██▉                                      | 50/702 [00:00<00:01, 496.90it/s]\u001b[A\u001b[A\n",
      "\n",
      " 14%|█████▋                                  | 100/702 [00:00<00:01, 494.24it/s]\u001b[A\u001b[A\n",
      "\n",
      " 21%|████████▌                               | 150/702 [00:00<00:01, 494.44it/s]\u001b[A\u001b[A\n",
      "\n",
      " 28%|███████████▍                            | 200/702 [00:00<00:01, 495.74it/s]\u001b[A\u001b[A\n",
      "\n",
      " 36%|██████████████▏                         | 250/702 [00:00<00:00, 494.82it/s]\u001b[A\u001b[A\n",
      "\n",
      " 43%|█████████████████                       | 300/702 [00:00<00:00, 488.27it/s]\u001b[A\u001b[A\n",
      "\n",
      " 50%|███████████████████▉                    | 349/702 [00:00<00:00, 488.16it/s]\u001b[A\u001b[A\n",
      "\n",
      " 57%|██████████████████████▋                 | 398/702 [00:00<00:00, 485.50it/s]\u001b[A\u001b[A\n",
      "\n",
      " 64%|█████████████████████████▍              | 447/702 [00:00<00:00, 480.87it/s]\u001b[A\u001b[A\n",
      "\n",
      " 71%|████████████████████████████▎           | 496/702 [00:01<00:00, 479.77it/s]\u001b[A\u001b[A\n",
      "\n",
      " 78%|███████████████████████████████         | 546/702 [00:01<00:00, 483.72it/s]\u001b[A\u001b[A\n",
      "\n",
      " 85%|█████████████████████████████████▉      | 595/702 [00:01<00:00, 479.59it/s]\u001b[A\u001b[A\n",
      "\n",
      " 92%|████████████████████████████████████▋   | 643/702 [00:01<00:00, 479.27it/s]\u001b[A\u001b[A\n",
      "\n",
      "100%|████████████████████████████████████████| 702/702 [00:01<00:00, 480.78it/s]\u001b[A\u001b[A\n",
      "10/09/2022 18:05:59 - INFO - utils_qa -   Saving predictions to ./run/predictions.json.\n",
      "10/09/2022 18:05:59 - INFO - utils_qa -   Saving nbest_preds to ./run/nbest_predictions.json.\n",
      "                                                                                \n",
      "\u001b[A{'exact_match': 26.923076923076923, 'f1': 66.19513669692562, 'epoch': 1.0}   \n",
      " 20%|███████▏                            | 6170/30850 [48:46<2:56:05,  2.34it/s]\n",
      "100%|███████████████████████████████████████████| 89/89 [00:14<00:00,  7.01it/s]\u001b[A\n",
      "{'loss': 1.8625, 'learning_rate': 2.4069192751235586e-05, 'epoch': 1.05}        \u001b[A\n",
      "{'loss': 1.7797, 'learning_rate': 2.357495881383855e-05, 'epoch': 1.13}         \n",
      "{'loss': 1.7485, 'learning_rate': 2.3080724876441514e-05, 'epoch': 1.22}        \n",
      "{'loss': 1.7781, 'learning_rate': 2.258649093904448e-05, 'epoch': 1.3}          \n",
      "{'loss': 1.7402, 'learning_rate': 2.2092257001647446e-05, 'epoch': 1.38}        \n",
      "{'loss': 1.7024, 'learning_rate': 2.1598023064250412e-05, 'epoch': 1.46}        \n",
      "{'loss': 1.7276, 'learning_rate': 2.1103789126853378e-05, 'epoch': 1.54}        \n",
      "{'loss': 1.7343, 'learning_rate': 2.0609555189456344e-05, 'epoch': 1.62}        \n",
      " 32%|██████████▋                      | 10000/30850 [1:18:25<2:39:26,  2.18it/s][INFO|trainer.py:2640] 2022-10-09 18:35:38,789 >> Saving model checkpoint to ./run/checkpoint-10000\n",
      "[INFO|configuration_utils.py:451] 2022-10-09 18:35:38,790 >> Configuration saved in ./run/checkpoint-10000/config.json\n",
      "[INFO|modeling_utils.py:1566] 2022-10-09 18:35:39,377 >> Model weights saved in ./run/checkpoint-10000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2145] 2022-10-09 18:35:39,378 >> tokenizer config file saved in ./run/checkpoint-10000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2152] 2022-10-09 18:35:39,378 >> Special tokens file saved in ./run/checkpoint-10000/special_tokens_map.json\n",
      "{'loss': 1.6984, 'learning_rate': 2.0115321252059307e-05, 'epoch': 1.7}         \n",
      "{'loss': 1.6534, 'learning_rate': 1.9621087314662276e-05, 'epoch': 1.78}        \n",
      "{'loss': 1.7376, 'learning_rate': 1.912685337726524e-05, 'epoch': 1.86}         \n",
      "{'loss': 1.6845, 'learning_rate': 1.8632619439868205e-05, 'epoch': 1.94}        \n",
      " 40%|█████████████▏                   | 12340/30850 [1:36:55<2:13:50,  2.30it/s][INFO|trainer.py:723] 2022-10-09 18:54:08,269 >> The following columns in the evaluation set don't have a corresponding argument in `ElectraForQuestionAnswering.forward` and have been ignored: offset_mapping, example_id. If offset_mapping, example_id are not expected by `ElectraForQuestionAnswering.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:3452] 2022-10-09 18:54:08,271 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:3453] 2022-10-09 18:54:08,271 >>   Num examples = 708\n",
      "[INFO|trainer.py:3454] 2022-10-09 18:54:08,271 >>   Batch size = 8\n",
      "/home/mohammed/anaconda3/envs/python37/lib/python3.7/site-packages/transformers/trainer_pt_utils.py:394: FutureWarning: DistributedTensorGatherer is deprecated and will be removed in v5 of Transformers.\n",
      "  FutureWarning,\n",
      "\n",
      "  0%|                                                    | 0/89 [00:00<?, ?it/s]\u001b[A\n",
      "  2%|▉                                           | 2/89 [00:00<00:06, 14.18it/s]\u001b[A\n",
      "  4%|█▉                                          | 4/89 [00:00<00:09,  8.90it/s]\u001b[A\n",
      "  6%|██▍                                         | 5/89 [00:00<00:10,  8.26it/s]\u001b[A\n",
      "  7%|██▉                                         | 6/89 [00:00<00:10,  7.85it/s]\u001b[A\n",
      "  8%|███▍                                        | 7/89 [00:00<00:10,  7.58it/s]\u001b[A\n",
      "  9%|███▉                                        | 8/89 [00:00<00:10,  7.41it/s]\u001b[A\n",
      " 10%|████▍                                       | 9/89 [00:01<00:10,  7.31it/s]\u001b[A\n",
      " 11%|████▊                                      | 10/89 [00:01<00:10,  7.22it/s]\u001b[A\n",
      " 12%|█████▎                                     | 11/89 [00:01<00:10,  7.17it/s]\u001b[A\n",
      " 13%|█████▊                                     | 12/89 [00:01<00:10,  7.14it/s]\u001b[A\n",
      " 15%|██████▎                                    | 13/89 [00:01<00:10,  7.11it/s]\u001b[A\n",
      " 16%|██████▊                                    | 14/89 [00:01<00:10,  7.09it/s]\u001b[A\n",
      " 17%|███████▏                                   | 15/89 [00:01<00:10,  7.08it/s]\u001b[A\n",
      " 18%|███████▋                                   | 16/89 [00:02<00:10,  7.07it/s]\u001b[A\n",
      " 19%|████████▏                                  | 17/89 [00:02<00:10,  7.06it/s]\u001b[A\n",
      " 20%|████████▋                                  | 18/89 [00:02<00:10,  7.06it/s]\u001b[A\n",
      " 21%|█████████▏                                 | 19/89 [00:02<00:09,  7.06it/s]\u001b[A\n",
      " 22%|█████████▋                                 | 20/89 [00:02<00:09,  7.06it/s]\u001b[A\n",
      " 24%|██████████▏                                | 21/89 [00:02<00:09,  7.06it/s]\u001b[A\n",
      " 25%|██████████▋                                | 22/89 [00:02<00:09,  7.05it/s]\u001b[A\n",
      " 26%|███████████                                | 23/89 [00:03<00:09,  7.05it/s]\u001b[A\n",
      " 27%|███████████▌                               | 24/89 [00:03<00:09,  7.03it/s]\u001b[A\n",
      " 28%|████████████                               | 25/89 [00:03<00:09,  7.06it/s]\u001b[A\n",
      " 29%|████████████▌                              | 26/89 [00:03<00:08,  7.06it/s]\u001b[A\n",
      " 30%|█████████████                              | 27/89 [00:03<00:08,  7.05it/s]\u001b[A\n",
      " 31%|█████████████▌                             | 28/89 [00:03<00:08,  7.04it/s]\u001b[A\n",
      " 33%|██████████████                             | 29/89 [00:03<00:08,  7.05it/s]\u001b[A\n",
      " 34%|██████████████▍                            | 30/89 [00:04<00:08,  7.04it/s]\u001b[A\n",
      " 35%|██████████████▉                            | 31/89 [00:04<00:08,  7.05it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 32/89 [00:04<00:08,  7.05it/s]\u001b[A\n",
      " 37%|███████████████▉                           | 33/89 [00:04<00:07,  7.05it/s]\u001b[A\n",
      " 38%|████████████████▍                          | 34/89 [00:04<00:07,  7.05it/s]\u001b[A\n",
      " 39%|████████████████▉                          | 35/89 [00:04<00:07,  7.05it/s]\u001b[A\n",
      " 40%|█████████████████▍                         | 36/89 [00:04<00:07,  7.05it/s]\u001b[A\n",
      " 42%|█████████████████▉                         | 37/89 [00:05<00:07,  7.05it/s]\u001b[A\n",
      " 43%|██████████████████▎                        | 38/89 [00:05<00:07,  7.05it/s]\u001b[A\n",
      " 44%|██████████████████▊                        | 39/89 [00:05<00:07,  7.04it/s]\u001b[A\n",
      " 45%|███████████████████▎                       | 40/89 [00:05<00:06,  7.04it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 41/89 [00:05<00:06,  7.05it/s]\u001b[A\n",
      " 47%|████████████████████▎                      | 42/89 [00:05<00:06,  7.05it/s]\u001b[A\n",
      " 48%|████████████████████▊                      | 43/89 [00:05<00:06,  7.06it/s]\u001b[A\n",
      " 49%|█████████████████████▎                     | 44/89 [00:06<00:06,  7.05it/s]\u001b[A\n",
      " 51%|█████████████████████▋                     | 45/89 [00:06<00:06,  7.06it/s]\u001b[A\n",
      " 52%|██████████████████████▏                    | 46/89 [00:06<00:06,  7.05it/s]\u001b[A\n",
      " 53%|██████████████████████▋                    | 47/89 [00:06<00:05,  7.05it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 48/89 [00:06<00:05,  7.05it/s]\u001b[A\n",
      " 55%|███████████████████████▋                   | 49/89 [00:06<00:05,  7.04it/s]\u001b[A\n",
      " 56%|████████████████████████▏                  | 50/89 [00:06<00:05,  7.05it/s]\u001b[A\n",
      " 57%|████████████████████████▋                  | 51/89 [00:07<00:05,  7.05it/s]\u001b[A\n",
      " 58%|█████████████████████████                  | 52/89 [00:07<00:05,  7.00it/s]\u001b[A\n",
      " 60%|█████████████████████████▌                 | 53/89 [00:07<00:05,  7.02it/s]\u001b[A\n",
      " 61%|██████████████████████████                 | 54/89 [00:07<00:04,  7.03it/s]\u001b[A\n",
      " 62%|██████████████████████████▌                | 55/89 [00:07<00:04,  7.04it/s]\u001b[A\n",
      " 63%|███████████████████████████                | 56/89 [00:07<00:04,  7.05it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 57/89 [00:07<00:04,  7.05it/s]\u001b[A\n",
      " 65%|████████████████████████████               | 58/89 [00:08<00:04,  7.05it/s]\u001b[A\n",
      " 66%|████████████████████████████▌              | 59/89 [00:08<00:04,  7.04it/s]\u001b[A\n",
      " 67%|████████████████████████████▉              | 60/89 [00:08<00:04,  7.05it/s]\u001b[A\n",
      " 69%|█████████████████████████████▍             | 61/89 [00:08<00:03,  7.05it/s]\u001b[A\n",
      " 70%|█████████████████████████████▉             | 62/89 [00:08<00:03,  7.05it/s]\u001b[A\n",
      " 71%|██████████████████████████████▍            | 63/89 [00:08<00:03,  7.05it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 64/89 [00:08<00:03,  7.05it/s]\u001b[A\n",
      " 73%|███████████████████████████████▍           | 65/89 [00:09<00:03,  7.06it/s]\u001b[A\n",
      " 74%|███████████████████████████████▉           | 66/89 [00:09<00:03,  7.06it/s]\u001b[A\n",
      " 75%|████████████████████████████████▎          | 67/89 [00:09<00:03,  7.06it/s]\u001b[A\n",
      " 76%|████████████████████████████████▊          | 68/89 [00:09<00:02,  7.06it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▎         | 69/89 [00:09<00:02,  7.06it/s]\u001b[A\n",
      " 79%|█████████████████████████████████▊         | 70/89 [00:09<00:02,  7.06it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▎        | 71/89 [00:09<00:02,  7.06it/s]\u001b[A\n",
      " 81%|██████████████████████████████████▊        | 72/89 [00:10<00:02,  7.06it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 73/89 [00:10<00:02,  7.06it/s]\u001b[A\n",
      " 83%|███████████████████████████████████▊       | 74/89 [00:10<00:02,  7.05it/s]\u001b[A\n",
      " 84%|████████████████████████████████████▏      | 75/89 [00:10<00:01,  7.05it/s]\u001b[A\n",
      " 85%|████████████████████████████████████▋      | 76/89 [00:10<00:01,  7.05it/s]\u001b[A\n",
      " 87%|█████████████████████████████████████▏     | 77/89 [00:10<00:01,  7.05it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▋     | 78/89 [00:10<00:01,  7.05it/s]\u001b[A\n",
      " 89%|██████████████████████████████████████▏    | 79/89 [00:11<00:01,  7.04it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▋    | 80/89 [00:11<00:01,  7.05it/s]\u001b[A\n",
      " 91%|███████████████████████████████████████▏   | 81/89 [00:11<00:01,  7.04it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 82/89 [00:11<00:00,  7.02it/s]\u001b[A\n",
      " 93%|████████████████████████████████████████   | 83/89 [00:11<00:00,  7.05it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▌  | 84/89 [00:11<00:00,  7.05it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████  | 85/89 [00:11<00:00,  7.06it/s]\u001b[A\n",
      " 97%|█████████████████████████████████████████▌ | 86/89 [00:12<00:00,  7.05it/s]\u001b[A\n",
      " 98%|██████████████████████████████████████████ | 87/89 [00:12<00:00,  7.05it/s]\u001b[A\n",
      " 99%|██████████████████████████████████████████▌| 88/89 [00:12<00:00,  7.05it/s]\u001b[A\n",
      "100%|███████████████████████████████████████████| 89/89 [00:12<00:00,  7.05it/s]\u001b[A10/09/2022 18:54:21 - INFO - utils_qa -   Post-processing 702 example predictions split into 708 features.\n",
      "\n",
      "\n",
      "  0%|                                                   | 0/702 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "  7%|██▉                                      | 51/702 [00:00<00:01, 501.64it/s]\u001b[A\u001b[A\n",
      "\n",
      " 15%|█████▊                                  | 102/702 [00:00<00:01, 502.74it/s]\u001b[A\u001b[A\n",
      "\n",
      " 22%|████████▋                               | 153/702 [00:00<00:01, 504.37it/s]\u001b[A\u001b[A\n",
      "\n",
      " 29%|███████████▋                            | 205/702 [00:00<00:00, 506.78it/s]\u001b[A\u001b[A\n",
      "\n",
      " 36%|██████████████▌                         | 256/702 [00:00<00:00, 506.97it/s]\u001b[A\u001b[A\n",
      "\n",
      " 44%|█████████████████▍                      | 307/702 [00:00<00:00, 502.53it/s]\u001b[A\u001b[A\n",
      "\n",
      " 51%|████████████████████▍                   | 358/702 [00:00<00:00, 504.02it/s]\u001b[A\u001b[A\n",
      "\n",
      " 58%|███████████████████████▎                | 409/702 [00:00<00:00, 501.64it/s]\u001b[A\u001b[A\n",
      "\n",
      " 66%|██████████████████████████▏             | 460/702 [00:00<00:00, 494.79it/s]\u001b[A\u001b[A\n",
      "\n",
      " 73%|█████████████████████████████           | 510/702 [00:01<00:00, 495.16it/s]\u001b[A\u001b[A\n",
      "\n",
      " 80%|███████████████████████████████▉        | 561/702 [00:01<00:00, 496.64it/s]\u001b[A\u001b[A\n",
      "\n",
      " 87%|██████████████████████████████████▊     | 611/702 [00:01<00:00, 493.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "100%|████████████████████████████████████████| 702/702 [00:01<00:00, 494.70it/s]\u001b[A\u001b[A\n",
      "10/09/2022 18:54:22 - INFO - utils_qa -   Saving predictions to ./run/predictions.json.\n",
      "10/09/2022 18:54:22 - INFO - utils_qa -   Saving nbest_preds to ./run/nbest_predictions.json.\n",
      "                                                                                \n",
      "\u001b[A{'exact_match': 35.32763532763533, 'f1': 70.8766813007773, 'epoch': 2.0}     \n",
      " 40%|█████████████▏                   | 12340/30850 [1:37:10<2:13:50,  2.30it/s]\n",
      "100%|███████████████████████████████████████████| 89/89 [00:14<00:00,  7.05it/s]\u001b[A\n",
      "{'loss': 1.5827, 'learning_rate': 1.813838550247117e-05, 'epoch': 2.03}         \u001b[A\n",
      "{'loss': 1.2868, 'learning_rate': 1.7644151565074136e-05, 'epoch': 2.11}        \n",
      "{'loss': 1.2966, 'learning_rate': 1.7149917627677102e-05, 'epoch': 2.19}        \n",
      "{'loss': 1.2922, 'learning_rate': 1.6655683690280065e-05, 'epoch': 2.27}        \n",
      "{'loss': 1.297, 'learning_rate': 1.616144975288303e-05, 'epoch': 2.35}          \n",
      "{'loss': 1.3389, 'learning_rate': 1.5667215815485997e-05, 'epoch': 2.43}        \n",
      "{'loss': 1.2744, 'learning_rate': 1.5172981878088963e-05, 'epoch': 2.51}        \n",
      "{'loss': 1.2751, 'learning_rate': 1.4678747940691929e-05, 'epoch': 2.59}        \n",
      "{'loss': 1.3154, 'learning_rate': 1.4184514003294893e-05, 'epoch': 2.67}        \n",
      "{'loss': 1.307, 'learning_rate': 1.369028006589786e-05, 'epoch': 2.76}          \n",
      "{'loss': 1.2977, 'learning_rate': 1.3196046128500823e-05, 'epoch': 2.84}        \n",
      "{'loss': 1.324, 'learning_rate': 1.270181219110379e-05, 'epoch': 2.92}          \n",
      "{'loss': 1.3121, 'learning_rate': 1.2207578253706754e-05, 'epoch': 3.0}         \n",
      " 60%|███████████████████▊             | 18510/30850 [2:24:18<1:25:15,  2.41it/s][INFO|trainer.py:723] 2022-10-09 19:41:31,713 >> The following columns in the evaluation set don't have a corresponding argument in `ElectraForQuestionAnswering.forward` and have been ignored: offset_mapping, example_id. If offset_mapping, example_id are not expected by `ElectraForQuestionAnswering.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:3452] 2022-10-09 19:41:31,714 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:3453] 2022-10-09 19:41:31,714 >>   Num examples = 708\n",
      "[INFO|trainer.py:3454] 2022-10-09 19:41:31,714 >>   Batch size = 8\n",
      "\n",
      "  0%|                                                    | 0/89 [00:00<?, ?it/s]\u001b[A\n",
      "  2%|▉                                           | 2/89 [00:00<00:06, 14.32it/s]\u001b[A\n",
      "  4%|█▉                                          | 4/89 [00:00<00:09,  8.78it/s]\u001b[A\n",
      "  6%|██▍                                         | 5/89 [00:00<00:10,  8.18it/s]\u001b[A\n",
      "  7%|██▉                                         | 6/89 [00:00<00:10,  7.81it/s]\u001b[A\n",
      "  8%|███▍                                        | 7/89 [00:00<00:10,  7.56it/s]\u001b[A\n",
      "  9%|███▉                                        | 8/89 [00:00<00:10,  7.40it/s]\u001b[A\n",
      " 10%|████▍                                       | 9/89 [00:01<00:10,  7.28it/s]\u001b[A\n",
      " 11%|████▊                                      | 10/89 [00:01<00:10,  7.21it/s]\u001b[A\n",
      " 12%|█████▎                                     | 11/89 [00:01<00:10,  7.17it/s]\u001b[A\n",
      " 13%|█████▊                                     | 12/89 [00:01<00:10,  7.13it/s]\u001b[A\n",
      " 15%|██████▎                                    | 13/89 [00:01<00:10,  7.10it/s]\u001b[A\n",
      " 16%|██████▊                                    | 14/89 [00:01<00:10,  7.08it/s]\u001b[A\n",
      " 17%|███████▏                                   | 15/89 [00:01<00:10,  7.07it/s]\u001b[A\n",
      " 18%|███████▋                                   | 16/89 [00:02<00:10,  7.07it/s]\u001b[A\n",
      " 19%|████████▏                                  | 17/89 [00:02<00:10,  7.07it/s]\u001b[A\n",
      " 20%|████████▋                                  | 18/89 [00:02<00:10,  7.06it/s]\u001b[A\n",
      " 21%|█████████▏                                 | 19/89 [00:02<00:09,  7.06it/s]\u001b[A\n",
      " 22%|█████████▋                                 | 20/89 [00:02<00:09,  7.06it/s]\u001b[A\n",
      " 24%|██████████▏                                | 21/89 [00:02<00:09,  7.06it/s]\u001b[A\n",
      " 25%|██████████▋                                | 22/89 [00:02<00:09,  7.05it/s]\u001b[A\n",
      " 26%|███████████                                | 23/89 [00:03<00:09,  7.05it/s]\u001b[A\n",
      " 27%|███████████▌                               | 24/89 [00:03<00:09,  7.05it/s]\u001b[A\n",
      " 28%|████████████                               | 25/89 [00:03<00:09,  7.05it/s]\u001b[A\n",
      " 29%|████████████▌                              | 26/89 [00:03<00:08,  7.04it/s]\u001b[A\n",
      " 30%|█████████████                              | 27/89 [00:03<00:08,  7.04it/s]\u001b[A\n",
      " 31%|█████████████▌                             | 28/89 [00:03<00:08,  7.04it/s]\u001b[A\n",
      " 33%|██████████████                             | 29/89 [00:03<00:08,  7.04it/s]\u001b[A\n",
      " 34%|██████████████▍                            | 30/89 [00:04<00:08,  7.03it/s]\u001b[A\n",
      " 35%|██████████████▉                            | 31/89 [00:04<00:08,  7.02it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 32/89 [00:04<00:08,  6.86it/s]\u001b[A\n",
      " 37%|███████████████▉                           | 33/89 [00:04<00:08,  6.58it/s]\u001b[A\n",
      " 38%|████████████████▍                          | 34/89 [00:04<00:08,  6.49it/s]\u001b[A\n",
      " 39%|████████████████▉                          | 35/89 [00:04<00:08,  6.60it/s]\u001b[A\n",
      " 40%|█████████████████▍                         | 36/89 [00:05<00:07,  6.68it/s]\u001b[A\n",
      " 42%|█████████████████▉                         | 37/89 [00:05<00:07,  6.74it/s]\u001b[A\n",
      " 43%|██████████████████▎                        | 38/89 [00:05<00:07,  6.80it/s]\u001b[A\n",
      " 44%|██████████████████▊                        | 39/89 [00:05<00:07,  6.85it/s]\u001b[A\n",
      " 45%|███████████████████▎                       | 40/89 [00:05<00:07,  6.75it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 41/89 [00:05<00:07,  6.77it/s]\u001b[A\n",
      " 47%|████████████████████▎                      | 42/89 [00:05<00:06,  6.80it/s]\u001b[A\n",
      " 48%|████████████████████▊                      | 43/89 [00:06<00:06,  6.62it/s]\u001b[A\n",
      " 49%|█████████████████████▎                     | 44/89 [00:06<00:06,  6.64it/s]\u001b[A\n",
      " 51%|█████████████████████▋                     | 45/89 [00:06<00:06,  6.59it/s]\u001b[A\n",
      " 52%|██████████████████████▏                    | 46/89 [00:06<00:06,  6.62it/s]\u001b[A\n",
      " 53%|██████████████████████▋                    | 47/89 [00:06<00:06,  6.70it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 48/89 [00:06<00:06,  6.65it/s]\u001b[A\n",
      " 55%|███████████████████████▋                   | 49/89 [00:06<00:05,  6.71it/s]\u001b[A\n",
      " 56%|████████████████████████▏                  | 50/89 [00:07<00:05,  6.79it/s]\u001b[A\n",
      " 57%|████████████████████████▋                  | 51/89 [00:07<00:05,  6.86it/s]\u001b[A\n",
      " 58%|█████████████████████████                  | 52/89 [00:07<00:05,  6.92it/s]\u001b[A\n",
      " 60%|█████████████████████████▌                 | 53/89 [00:07<00:05,  6.95it/s]\u001b[A\n",
      " 61%|██████████████████████████                 | 54/89 [00:07<00:05,  6.88it/s]\u001b[A\n",
      " 62%|██████████████████████████▌                | 55/89 [00:07<00:04,  6.93it/s]\u001b[A\n",
      " 63%|███████████████████████████                | 56/89 [00:07<00:04,  6.96it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 57/89 [00:08<00:04,  6.85it/s]\u001b[A\n",
      " 65%|████████████████████████████               | 58/89 [00:08<00:04,  6.82it/s]\u001b[A\n",
      " 66%|████████████████████████████▌              | 59/89 [00:08<00:04,  6.84it/s]\u001b[A\n",
      " 67%|████████████████████████████▉              | 60/89 [00:08<00:04,  6.87it/s]\u001b[A\n",
      " 69%|█████████████████████████████▍             | 61/89 [00:08<00:04,  6.92it/s]\u001b[A\n",
      " 70%|█████████████████████████████▉             | 62/89 [00:08<00:03,  6.95it/s]\u001b[A\n",
      " 71%|██████████████████████████████▍            | 63/89 [00:08<00:03,  6.98it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 64/89 [00:09<00:03,  7.00it/s]\u001b[A\n",
      " 73%|███████████████████████████████▍           | 65/89 [00:09<00:03,  7.01it/s]\u001b[A\n",
      " 74%|███████████████████████████████▉           | 66/89 [00:09<00:03,  7.02it/s]\u001b[A\n",
      " 75%|████████████████████████████████▎          | 67/89 [00:09<00:03,  7.03it/s]\u001b[A\n",
      " 76%|████████████████████████████████▊          | 68/89 [00:09<00:02,  7.03it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▎         | 69/89 [00:09<00:02,  7.04it/s]\u001b[A\n",
      " 79%|█████████████████████████████████▊         | 70/89 [00:09<00:02,  7.04it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▎        | 71/89 [00:10<00:02,  7.05it/s]\u001b[A\n",
      " 81%|██████████████████████████████████▊        | 72/89 [00:10<00:02,  7.03it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 73/89 [00:10<00:02,  6.97it/s]\u001b[A\n",
      " 83%|███████████████████████████████████▊       | 74/89 [00:10<00:02,  6.95it/s]\u001b[A\n",
      " 84%|████████████████████████████████████▏      | 75/89 [00:10<00:02,  6.93it/s]\u001b[A\n",
      " 85%|████████████████████████████████████▋      | 76/89 [00:10<00:01,  6.91it/s]\u001b[A\n",
      " 87%|█████████████████████████████████████▏     | 77/89 [00:10<00:01,  6.90it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▋     | 78/89 [00:11<00:01,  6.91it/s]\u001b[A\n",
      " 89%|██████████████████████████████████████▏    | 79/89 [00:11<00:01,  6.95it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▋    | 80/89 [00:11<00:01,  6.97it/s]\u001b[A\n",
      " 91%|███████████████████████████████████████▏   | 81/89 [00:11<00:01,  6.99it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 82/89 [00:11<00:01,  6.91it/s]\u001b[A\n",
      " 93%|████████████████████████████████████████   | 83/89 [00:11<00:00,  6.89it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▌  | 84/89 [00:12<00:00,  6.88it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████  | 85/89 [00:12<00:00,  6.87it/s]\u001b[A\n",
      " 97%|█████████████████████████████████████████▌ | 86/89 [00:12<00:00,  6.89it/s]\u001b[A\n",
      " 98%|██████████████████████████████████████████ | 87/89 [00:12<00:00,  6.93it/s]\u001b[A\n",
      " 99%|██████████████████████████████████████████▌| 88/89 [00:12<00:00,  6.96it/s]\u001b[A\n",
      "100%|███████████████████████████████████████████| 89/89 [00:12<00:00,  6.99it/s]\u001b[A10/09/2022 19:41:45 - INFO - utils_qa -   Post-processing 702 example predictions split into 708 features.\n",
      "\n",
      "\n",
      "  0%|                                                   | 0/702 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "  7%|██▉                                      | 50/702 [00:00<00:01, 495.86it/s]\u001b[A\u001b[A\n",
      "\n",
      " 14%|█████▊                                  | 101/702 [00:00<00:01, 499.83it/s]\u001b[A\u001b[A\n",
      "\n",
      " 22%|████████▋                               | 152/702 [00:00<00:01, 502.81it/s]\u001b[A\u001b[A\n",
      "\n",
      " 29%|███████████▌                            | 204/702 [00:00<00:00, 505.32it/s]\u001b[A\u001b[A\n",
      "\n",
      " 36%|██████████████▌                         | 255/702 [00:00<00:00, 505.77it/s]\u001b[A\u001b[A\n",
      "\n",
      " 44%|█████████████████▍                      | 306/702 [00:00<00:00, 501.22it/s]\u001b[A\u001b[A\n",
      "\n",
      " 51%|████████████████████▎                   | 357/702 [00:00<00:00, 502.53it/s]\u001b[A\u001b[A\n",
      "\n",
      " 58%|███████████████████████▏                | 408/702 [00:00<00:00, 496.14it/s]\u001b[A\u001b[A\n",
      "\n",
      " 65%|██████████████████████████              | 458/702 [00:00<00:00, 485.07it/s]\u001b[A\u001b[A\n",
      "\n",
      " 72%|████████████████████████████▉           | 507/702 [00:01<00:00, 485.20it/s]\u001b[A\u001b[A\n",
      "\n",
      " 79%|███████████████████████████████▋        | 556/702 [00:01<00:00, 486.52it/s]\u001b[A\u001b[A\n",
      "\n",
      " 86%|██████████████████████████████████▍     | 605/702 [00:01<00:00, 484.04it/s]\u001b[A\u001b[A\n",
      "\n",
      " 93%|█████████████████████████████████████▎  | 654/702 [00:01<00:00, 477.88it/s]\u001b[A\u001b[A\n",
      "\n",
      "100%|████████████████████████████████████████| 702/702 [00:01<00:00, 485.79it/s]\u001b[A\u001b[A\n",
      "10/09/2022 19:41:46 - INFO - utils_qa -   Saving predictions to ./run/predictions.json.\n",
      "10/09/2022 19:41:46 - INFO - utils_qa -   Saving nbest_preds to ./run/nbest_predictions.json.\n",
      "                                                                                \n",
      "\u001b[A{'exact_match': 35.18518518518518, 'f1': 70.37915470008213, 'epoch': 3.0}    \n",
      " 60%|███████████████████▊             | 18510/30850 [2:24:33<1:25:15,  2.41it/s]\n",
      "100%|███████████████████████████████████████████| 89/89 [00:15<00:00,  6.99it/s]\u001b[A\n",
      "{'loss': 0.9658, 'learning_rate': 1.171334431630972e-05, 'epoch': 3.08}         \u001b[A\n",
      "{'loss': 0.9621, 'learning_rate': 1.1219110378912686e-05, 'epoch': 3.16}        \n",
      "{'loss': 0.9309, 'learning_rate': 1.0724876441515652e-05, 'epoch': 3.24}        \n",
      " 65%|█████████████████████▍           | 20000/30850 [2:37:23<1:34:22,  1.92it/s][INFO|trainer.py:2640] 2022-10-09 19:54:36,784 >> Saving model checkpoint to ./run/checkpoint-20000\n",
      "[INFO|configuration_utils.py:451] 2022-10-09 19:54:36,785 >> Configuration saved in ./run/checkpoint-20000/config.json\n",
      "[INFO|modeling_utils.py:1566] 2022-10-09 19:54:37,454 >> Model weights saved in ./run/checkpoint-20000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2145] 2022-10-09 19:54:37,454 >> tokenizer config file saved in ./run/checkpoint-20000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2152] 2022-10-09 19:54:37,454 >> Special tokens file saved in ./run/checkpoint-20000/special_tokens_map.json\n",
      "{'loss': 0.9115, 'learning_rate': 1.0230642504118616e-05, 'epoch': 3.32}        \n",
      "{'loss': 0.9471, 'learning_rate': 9.736408566721582e-06, 'epoch': 3.4}          \n",
      "{'loss': 0.9945, 'learning_rate': 9.242174629324548e-06, 'epoch': 3.48}         \n",
      "{'loss': 0.9679, 'learning_rate': 8.747940691927512e-06, 'epoch': 3.57}         \n",
      "{'loss': 0.9906, 'learning_rate': 8.253706754530478e-06, 'epoch': 3.65}         \n",
      "{'loss': 0.9552, 'learning_rate': 7.759472817133444e-06, 'epoch': 3.73}         \n",
      "{'loss': 0.9709, 'learning_rate': 7.265238879736408e-06, 'epoch': 3.81}         \n",
      "{'loss': 0.9414, 'learning_rate': 6.771004942339374e-06, 'epoch': 3.89}         \n",
      "{'loss': 0.9392, 'learning_rate': 6.2767710049423394e-06, 'epoch': 3.97}        \n",
      " 80%|████████████████████████████       | 24680/30850 [3:15:19<44:48,  2.29it/s][INFO|trainer.py:723] 2022-10-09 20:32:32,208 >> The following columns in the evaluation set don't have a corresponding argument in `ElectraForQuestionAnswering.forward` and have been ignored: offset_mapping, example_id. If offset_mapping, example_id are not expected by `ElectraForQuestionAnswering.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:3452] 2022-10-09 20:32:32,209 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:3453] 2022-10-09 20:32:32,210 >>   Num examples = 708\n",
      "[INFO|trainer.py:3454] 2022-10-09 20:32:32,210 >>   Batch size = 8\n",
      "/home/mohammed/anaconda3/envs/python37/lib/python3.7/site-packages/transformers/trainer_pt_utils.py:394: FutureWarning: DistributedTensorGatherer is deprecated and will be removed in v5 of Transformers.\n",
      "  FutureWarning,\n",
      "\n",
      "  0%|                                                    | 0/89 [00:00<?, ?it/s]\u001b[A\n",
      "  2%|▉                                           | 2/89 [00:00<00:06, 13.78it/s]\u001b[A\n",
      "  4%|█▉                                          | 4/89 [00:00<00:09,  8.76it/s]\u001b[A\n",
      "  6%|██▍                                         | 5/89 [00:00<00:10,  8.17it/s]\u001b[A\n",
      "  7%|██▉                                         | 6/89 [00:00<00:10,  7.79it/s]\u001b[A\n",
      "  8%|███▍                                        | 7/89 [00:00<00:10,  7.56it/s]\u001b[A\n",
      "  9%|███▉                                        | 8/89 [00:01<00:11,  7.34it/s]\u001b[A\n",
      " 10%|████▍                                       | 9/89 [00:01<00:11,  7.25it/s]\u001b[A\n",
      " 11%|████▊                                      | 10/89 [00:01<00:11,  7.18it/s]\u001b[A\n",
      " 12%|█████▎                                     | 11/89 [00:01<00:10,  7.14it/s]\u001b[A\n",
      " 13%|█████▊                                     | 12/89 [00:01<00:10,  7.06it/s]\u001b[A\n",
      " 15%|██████▎                                    | 13/89 [00:01<00:10,  7.10it/s]\u001b[A\n",
      " 16%|██████▊                                    | 14/89 [00:01<00:10,  7.08it/s]\u001b[A\n",
      " 17%|███████▏                                   | 15/89 [00:01<00:10,  7.07it/s]\u001b[A\n",
      " 18%|███████▋                                   | 16/89 [00:02<00:10,  7.06it/s]\u001b[A\n",
      " 19%|████████▏                                  | 17/89 [00:02<00:10,  7.05it/s]\u001b[A\n",
      " 20%|████████▋                                  | 18/89 [00:02<00:10,  7.03it/s]\u001b[A\n",
      " 21%|█████████▏                                 | 19/89 [00:02<00:09,  7.00it/s]\u001b[A\n",
      " 22%|█████████▋                                 | 20/89 [00:02<00:09,  6.98it/s]\u001b[A\n",
      " 24%|██████████▏                                | 21/89 [00:02<00:09,  6.96it/s]\u001b[A\n",
      " 25%|██████████▋                                | 22/89 [00:03<00:09,  6.98it/s]\u001b[A\n",
      " 26%|███████████                                | 23/89 [00:03<00:09,  6.81it/s]\u001b[A\n",
      " 27%|███████████▌                               | 24/89 [00:03<00:09,  6.82it/s]\u001b[A\n",
      " 28%|████████████                               | 25/89 [00:03<00:09,  6.79it/s]\u001b[A\n",
      " 29%|████████████▌                              | 26/89 [00:03<00:09,  6.41it/s]\u001b[A\n",
      " 30%|█████████████                              | 27/89 [00:03<00:09,  6.46it/s]\u001b[A\n",
      " 31%|█████████████▌                             | 28/89 [00:03<00:09,  6.61it/s]\u001b[A\n",
      " 33%|██████████████                             | 29/89 [00:04<00:08,  6.70it/s]\u001b[A\n",
      " 34%|██████████████▍                            | 30/89 [00:04<00:08,  6.77it/s]\u001b[A\n",
      " 35%|██████████████▉                            | 31/89 [00:04<00:08,  6.84it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 32/89 [00:04<00:08,  6.85it/s]\u001b[A\n",
      " 37%|███████████████▉                           | 33/89 [00:04<00:08,  6.92it/s]\u001b[A\n",
      " 38%|████████████████▍                          | 34/89 [00:04<00:07,  6.95it/s]\u001b[A\n",
      " 39%|████████████████▉                          | 35/89 [00:04<00:07,  6.96it/s]\u001b[A\n",
      " 40%|█████████████████▍                         | 36/89 [00:05<00:07,  6.99it/s]\u001b[A\n",
      " 42%|█████████████████▉                         | 37/89 [00:05<00:07,  7.01it/s]\u001b[A\n",
      " 43%|██████████████████▎                        | 38/89 [00:05<00:07,  7.02it/s]\u001b[A\n",
      " 44%|██████████████████▊                        | 39/89 [00:05<00:07,  7.03it/s]\u001b[A\n",
      " 45%|███████████████████▎                       | 40/89 [00:05<00:06,  7.04it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 41/89 [00:05<00:06,  7.03it/s]\u001b[A\n",
      " 47%|████████████████████▎                      | 42/89 [00:05<00:06,  7.04it/s]\u001b[A\n",
      " 48%|████████████████████▊                      | 43/89 [00:06<00:06,  7.01it/s]\u001b[A\n",
      " 49%|█████████████████████▎                     | 44/89 [00:06<00:06,  7.02it/s]\u001b[A\n",
      " 51%|█████████████████████▋                     | 45/89 [00:06<00:06,  7.03it/s]\u001b[A\n",
      " 52%|██████████████████████▏                    | 46/89 [00:06<00:06,  7.03it/s]\u001b[A\n",
      " 53%|██████████████████████▋                    | 47/89 [00:06<00:05,  7.03it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 48/89 [00:06<00:05,  7.04it/s]\u001b[A\n",
      " 55%|███████████████████████▋                   | 49/89 [00:06<00:05,  7.05it/s]\u001b[A\n",
      " 56%|████████████████████████▏                  | 50/89 [00:07<00:05,  7.05it/s]\u001b[A\n",
      " 57%|████████████████████████▋                  | 51/89 [00:07<00:05,  7.05it/s]\u001b[A\n",
      " 58%|█████████████████████████                  | 52/89 [00:07<00:05,  7.04it/s]\u001b[A\n",
      " 60%|█████████████████████████▌                 | 53/89 [00:07<00:05,  7.05it/s]\u001b[A\n",
      " 61%|██████████████████████████                 | 54/89 [00:07<00:04,  7.04it/s]\u001b[A\n",
      " 62%|██████████████████████████▌                | 55/89 [00:07<00:04,  7.04it/s]\u001b[A\n",
      " 63%|███████████████████████████                | 56/89 [00:07<00:04,  7.04it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 57/89 [00:08<00:04,  7.05it/s]\u001b[A\n",
      " 65%|████████████████████████████               | 58/89 [00:08<00:04,  7.05it/s]\u001b[A\n",
      " 66%|████████████████████████████▌              | 59/89 [00:08<00:04,  7.05it/s]\u001b[A\n",
      " 67%|████████████████████████████▉              | 60/89 [00:08<00:04,  7.05it/s]\u001b[A\n",
      " 69%|█████████████████████████████▍             | 61/89 [00:08<00:03,  7.05it/s]\u001b[A\n",
      " 70%|█████████████████████████████▉             | 62/89 [00:08<00:03,  7.05it/s]\u001b[A\n",
      " 71%|██████████████████████████████▍            | 63/89 [00:08<00:03,  7.04it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 64/89 [00:09<00:03,  7.04it/s]\u001b[A\n",
      " 73%|███████████████████████████████▍           | 65/89 [00:09<00:03,  7.03it/s]\u001b[A\n",
      " 74%|███████████████████████████████▉           | 66/89 [00:09<00:03,  7.03it/s]\u001b[A\n",
      " 75%|████████████████████████████████▎          | 67/89 [00:09<00:03,  7.03it/s]\u001b[A\n",
      " 76%|████████████████████████████████▊          | 68/89 [00:09<00:02,  7.04it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▎         | 69/89 [00:09<00:02,  7.04it/s]\u001b[A\n",
      " 79%|█████████████████████████████████▊         | 70/89 [00:09<00:02,  6.99it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▎        | 71/89 [00:10<00:02,  7.00it/s]\u001b[A\n",
      " 81%|██████████████████████████████████▊        | 72/89 [00:10<00:02,  7.01it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 73/89 [00:10<00:02,  7.03it/s]\u001b[A\n",
      " 83%|███████████████████████████████████▊       | 74/89 [00:10<00:02,  7.03it/s]\u001b[A\n",
      " 84%|████████████████████████████████████▏      | 75/89 [00:10<00:02,  6.96it/s]\u001b[A\n",
      " 85%|████████████████████████████████████▋      | 76/89 [00:10<00:01,  6.94it/s]\u001b[A\n",
      " 87%|█████████████████████████████████████▏     | 77/89 [00:10<00:01,  6.97it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▋     | 78/89 [00:11<00:01,  6.96it/s]\u001b[A\n",
      " 89%|██████████████████████████████████████▏    | 79/89 [00:11<00:01,  6.78it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▋    | 80/89 [00:11<00:01,  6.83it/s]\u001b[A\n",
      " 91%|███████████████████████████████████████▏   | 81/89 [00:11<00:01,  6.89it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 82/89 [00:11<00:01,  6.94it/s]\u001b[A\n",
      " 93%|████████████████████████████████████████   | 83/89 [00:11<00:00,  6.97it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▌  | 84/89 [00:11<00:00,  7.00it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████  | 85/89 [00:12<00:00,  7.01it/s]\u001b[A\n",
      " 97%|█████████████████████████████████████████▌ | 86/89 [00:12<00:00,  7.02it/s]\u001b[A\n",
      " 98%|██████████████████████████████████████████ | 87/89 [00:12<00:00,  7.02it/s]\u001b[A\n",
      " 99%|██████████████████████████████████████████▌| 88/89 [00:12<00:00,  7.02it/s]\u001b[A\n",
      "100%|███████████████████████████████████████████| 89/89 [00:12<00:00,  7.03it/s]\u001b[A10/09/2022 20:32:45 - INFO - utils_qa -   Post-processing 702 example predictions split into 708 features.\n",
      "\n",
      "\n",
      "  0%|                                                   | 0/702 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "  7%|██▉                                      | 50/702 [00:00<00:01, 493.09it/s]\u001b[A\u001b[A\n",
      "\n",
      " 14%|█████▋                                  | 100/702 [00:00<00:01, 490.99it/s]\u001b[A\u001b[A\n",
      "\n",
      " 21%|████████▌                               | 150/702 [00:00<00:01, 494.09it/s]\u001b[A\u001b[A\n",
      "\n",
      " 28%|███████████▍                            | 200/702 [00:00<00:01, 495.50it/s]\u001b[A\u001b[A\n",
      "\n",
      " 36%|██████████████▏                         | 250/702 [00:00<00:00, 495.63it/s]\u001b[A\u001b[A\n",
      "\n",
      " 43%|█████████████████                       | 300/702 [00:00<00:00, 491.83it/s]\u001b[A\u001b[A\n",
      "\n",
      " 50%|███████████████████▉                    | 350/702 [00:00<00:00, 491.96it/s]\u001b[A\u001b[A\n",
      "\n",
      " 57%|██████████████████████▊                 | 400/702 [00:00<00:00, 489.25it/s]\u001b[A\u001b[A\n",
      "\n",
      " 64%|█████████████████████████▌              | 449/702 [00:00<00:00, 482.09it/s]\u001b[A\u001b[A\n",
      "\n",
      " 71%|████████████████████████████▍           | 498/702 [00:01<00:00, 476.49it/s]\u001b[A\u001b[A\n",
      "\n",
      " 78%|███████████████████████████████▏        | 547/702 [00:01<00:00, 478.48it/s]\u001b[A\u001b[A\n",
      "\n",
      " 85%|█████████████████████████████████▉      | 595/702 [00:01<00:00, 474.43it/s]\u001b[A\u001b[A\n",
      "\n",
      " 92%|████████████████████████████████████▋   | 643/702 [00:01<00:00, 474.89it/s]\u001b[A\u001b[A\n",
      "\n",
      "100%|████████████████████████████████████████| 702/702 [00:01<00:00, 479.13it/s]\u001b[A\u001b[A\n",
      "10/09/2022 20:32:46 - INFO - utils_qa -   Saving predictions to ./run/predictions.json.\n",
      "10/09/2022 20:32:46 - INFO - utils_qa -   Saving nbest_preds to ./run/nbest_predictions.json.\n",
      "                                                                                \n",
      "\u001b[A{'exact_match': 36.03988603988604, 'f1': 71.5497344775456, 'epoch': 4.0}     \n",
      " 80%|████████████████████████████       | 24680/30850 [3:15:34<44:48,  2.29it/s]\n",
      "100%|███████████████████████████████████████████| 89/89 [00:14<00:00,  7.03it/s]\u001b[A\n",
      "{'loss': 0.7626, 'learning_rate': 5.782537067545305e-06, 'epoch': 4.05}         \u001b[A\n",
      "{'loss': 0.7293, 'learning_rate': 5.2883031301482705e-06, 'epoch': 4.13}        \n",
      "{'loss': 0.7146, 'learning_rate': 4.794069192751236e-06, 'epoch': 4.21}         \n",
      "{'loss': 0.7336, 'learning_rate': 4.299835255354201e-06, 'epoch': 4.29}         \n",
      "{'loss': 0.6936, 'learning_rate': 3.8056013179571663e-06, 'epoch': 4.38}        \n",
      "{'loss': 0.7044, 'learning_rate': 3.311367380560132e-06, 'epoch': 4.46}         \n",
      "{'loss': 0.6967, 'learning_rate': 2.8171334431630974e-06, 'epoch': 4.54}        \n",
      "{'loss': 0.7053, 'learning_rate': 2.3228995057660626e-06, 'epoch': 4.62}        \n",
      "{'loss': 0.699, 'learning_rate': 1.8286655683690281e-06, 'epoch': 4.7}          \n",
      "{'loss': 0.6885, 'learning_rate': 1.3344316309719934e-06, 'epoch': 4.78}        \n",
      "{'loss': 0.7066, 'learning_rate': 8.401976935749589e-07, 'epoch': 4.86}         \n",
      " 97%|██████████████████████████████████ | 30000/30850 [3:55:35<06:31,  2.17it/s][INFO|trainer.py:2640] 2022-10-09 21:12:47,991 >> Saving model checkpoint to ./run/checkpoint-30000\n",
      "[INFO|configuration_utils.py:451] 2022-10-09 21:12:47,991 >> Configuration saved in ./run/checkpoint-30000/config.json\n",
      "[INFO|modeling_utils.py:1566] 2022-10-09 21:12:48,607 >> Model weights saved in ./run/checkpoint-30000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2145] 2022-10-09 21:12:48,607 >> tokenizer config file saved in ./run/checkpoint-30000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2152] 2022-10-09 21:12:48,607 >> Special tokens file saved in ./run/checkpoint-30000/special_tokens_map.json\n",
      "{'loss': 0.6762, 'learning_rate': 3.459637561779242e-07, 'epoch': 4.94}         \n",
      "100%|███████████████████████████████████| 30850/30850 [4:02:09<00:00,  2.55it/s][INFO|trainer.py:723] 2022-10-09 21:19:22,579 >> The following columns in the evaluation set don't have a corresponding argument in `ElectraForQuestionAnswering.forward` and have been ignored: offset_mapping, example_id. If offset_mapping, example_id are not expected by `ElectraForQuestionAnswering.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:3452] 2022-10-09 21:19:22,580 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:3453] 2022-10-09 21:19:22,580 >>   Num examples = 708\n",
      "[INFO|trainer.py:3454] 2022-10-09 21:19:22,580 >>   Batch size = 8\n",
      "/home/mohammed/anaconda3/envs/python37/lib/python3.7/site-packages/transformers/trainer_pt_utils.py:394: FutureWarning: DistributedTensorGatherer is deprecated and will be removed in v5 of Transformers.\n",
      "  FutureWarning,\n",
      "\n",
      "  0%|                                                    | 0/89 [00:00<?, ?it/s]\u001b[A\n",
      "  2%|▉                                           | 2/89 [00:00<00:04, 17.95it/s]\u001b[A\n",
      "  4%|█▉                                          | 4/89 [00:00<00:08, 10.00it/s]\u001b[A\n",
      "  7%|██▉                                         | 6/89 [00:00<00:09,  8.77it/s]\u001b[A\n",
      "  8%|███▍                                        | 7/89 [00:00<00:09,  8.46it/s]\u001b[A\n",
      "  9%|███▉                                        | 8/89 [00:00<00:09,  8.12it/s]\u001b[A\n",
      " 10%|████▍                                       | 9/89 [00:01<00:10,  7.95it/s]\u001b[A\n",
      " 11%|████▊                                      | 10/89 [00:01<00:10,  7.85it/s]\u001b[A\n",
      " 12%|█████▎                                     | 11/89 [00:01<00:10,  7.78it/s]\u001b[A\n",
      " 13%|█████▊                                     | 12/89 [00:01<00:09,  7.74it/s]\u001b[A\n",
      " 15%|██████▎                                    | 13/89 [00:01<00:09,  7.69it/s]\u001b[A\n",
      " 16%|██████▊                                    | 14/89 [00:01<00:09,  7.67it/s]\u001b[A\n",
      " 17%|███████▏                                   | 15/89 [00:01<00:09,  7.62it/s]\u001b[A\n",
      " 18%|███████▋                                   | 16/89 [00:01<00:09,  7.57it/s]\u001b[A\n",
      " 19%|████████▏                                  | 17/89 [00:02<00:09,  7.57it/s]\u001b[A\n",
      " 20%|████████▋                                  | 18/89 [00:02<00:09,  7.59it/s]\u001b[A\n",
      " 21%|█████████▏                                 | 19/89 [00:02<00:09,  7.58it/s]\u001b[A\n",
      " 22%|█████████▋                                 | 20/89 [00:02<00:09,  7.59it/s]\u001b[A\n",
      " 24%|██████████▏                                | 21/89 [00:02<00:08,  7.60it/s]\u001b[A\n",
      " 25%|██████████▋                                | 22/89 [00:02<00:08,  7.59it/s]\u001b[A\n",
      " 26%|███████████                                | 23/89 [00:02<00:08,  7.55it/s]\u001b[A\n",
      " 27%|███████████▌                               | 24/89 [00:03<00:08,  7.56it/s]\u001b[A\n",
      " 28%|████████████                               | 25/89 [00:03<00:08,  7.56it/s]\u001b[A\n",
      " 29%|████████████▌                              | 26/89 [00:03<00:08,  7.56it/s]\u001b[A\n",
      " 30%|█████████████                              | 27/89 [00:03<00:08,  7.56it/s]\u001b[A\n",
      " 31%|█████████████▌                             | 28/89 [00:03<00:08,  7.58it/s]\u001b[A\n",
      " 33%|██████████████                             | 29/89 [00:03<00:07,  7.58it/s]\u001b[A\n",
      " 34%|██████████████▍                            | 30/89 [00:03<00:07,  7.52it/s]\u001b[A\n",
      " 35%|██████████████▉                            | 31/89 [00:03<00:07,  7.54it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 32/89 [00:04<00:07,  7.55it/s]\u001b[A\n",
      " 37%|███████████████▉                           | 33/89 [00:04<00:07,  7.54it/s]\u001b[A\n",
      " 38%|████████████████▍                          | 34/89 [00:04<00:07,  7.55it/s]\u001b[A\n",
      " 39%|████████████████▉                          | 35/89 [00:04<00:07,  7.56it/s]\u001b[A\n",
      " 40%|█████████████████▍                         | 36/89 [00:04<00:07,  7.57it/s]\u001b[A\n",
      " 42%|█████████████████▉                         | 37/89 [00:04<00:06,  7.56it/s]\u001b[A\n",
      " 43%|██████████████████▎                        | 38/89 [00:04<00:06,  7.57it/s]\u001b[A\n",
      " 44%|██████████████████▊                        | 39/89 [00:05<00:06,  7.56it/s]\u001b[A\n",
      " 45%|███████████████████▎                       | 40/89 [00:05<00:06,  7.57it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 41/89 [00:05<00:06,  7.57it/s]\u001b[A\n",
      " 47%|████████████████████▎                      | 42/89 [00:05<00:06,  7.56it/s]\u001b[A\n",
      " 48%|████████████████████▊                      | 43/89 [00:05<00:06,  7.53it/s]\u001b[A\n",
      " 49%|█████████████████████▎                     | 44/89 [00:05<00:05,  7.55it/s]\u001b[A\n",
      " 51%|█████████████████████▋                     | 45/89 [00:05<00:05,  7.55it/s]\u001b[A\n",
      " 52%|██████████████████████▏                    | 46/89 [00:05<00:05,  7.56it/s]\u001b[A\n",
      " 53%|██████████████████████▋                    | 47/89 [00:06<00:05,  7.56it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 48/89 [00:06<00:05,  7.57it/s]\u001b[A\n",
      " 55%|███████████████████████▋                   | 49/89 [00:06<00:05,  7.56it/s]\u001b[A\n",
      " 56%|████████████████████████▏                  | 50/89 [00:06<00:05,  7.57it/s]\u001b[A\n",
      " 57%|████████████████████████▋                  | 51/89 [00:06<00:05,  7.55it/s]\u001b[A\n",
      " 58%|█████████████████████████                  | 52/89 [00:06<00:04,  7.55it/s]\u001b[A\n",
      " 60%|█████████████████████████▌                 | 53/89 [00:06<00:04,  7.34it/s]\u001b[A\n",
      " 61%|██████████████████████████                 | 54/89 [00:06<00:04,  7.41it/s]\u001b[A\n",
      " 62%|██████████████████████████▌                | 55/89 [00:07<00:04,  7.31it/s]\u001b[A\n",
      " 63%|███████████████████████████                | 56/89 [00:07<00:04,  7.23it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 57/89 [00:07<00:04,  7.32it/s]\u001b[A\n",
      " 65%|████████████████████████████               | 58/89 [00:07<00:04,  7.38it/s]\u001b[A\n",
      " 66%|████████████████████████████▌              | 59/89 [00:07<00:04,  7.43it/s]\u001b[A\n",
      " 67%|████████████████████████████▉              | 60/89 [00:07<00:03,  7.42it/s]\u001b[A\n",
      " 69%|█████████████████████████████▍             | 61/89 [00:07<00:03,  7.52it/s]\u001b[A\n",
      " 70%|█████████████████████████████▉             | 62/89 [00:08<00:03,  7.53it/s]\u001b[A\n",
      " 71%|██████████████████████████████▍            | 63/89 [00:08<00:03,  7.55it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 64/89 [00:08<00:03,  7.56it/s]\u001b[A\n",
      " 73%|███████████████████████████████▍           | 65/89 [00:08<00:03,  7.55it/s]\u001b[A\n",
      " 74%|███████████████████████████████▉           | 66/89 [00:08<00:03,  7.57it/s]\u001b[A\n",
      " 75%|████████████████████████████████▎          | 67/89 [00:08<00:03,  7.33it/s]\u001b[A\n",
      " 76%|████████████████████████████████▊          | 68/89 [00:08<00:02,  7.42it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▎         | 69/89 [00:09<00:02,  7.47it/s]\u001b[A\n",
      " 79%|█████████████████████████████████▊         | 70/89 [00:09<00:02,  7.48it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▎        | 71/89 [00:09<00:02,  7.53it/s]\u001b[A\n",
      " 81%|██████████████████████████████████▊        | 72/89 [00:09<00:02,  7.55it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 73/89 [00:09<00:02,  7.55it/s]\u001b[A\n",
      " 83%|███████████████████████████████████▊       | 74/89 [00:09<00:01,  7.54it/s]\u001b[A\n",
      " 84%|████████████████████████████████████▏      | 75/89 [00:09<00:01,  7.55it/s]\u001b[A\n",
      " 85%|████████████████████████████████████▋      | 76/89 [00:09<00:01,  7.55it/s]\u001b[A\n",
      " 87%|█████████████████████████████████████▏     | 77/89 [00:10<00:01,  7.55it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▋     | 78/89 [00:10<00:01,  7.55it/s]\u001b[A\n",
      " 89%|██████████████████████████████████████▏    | 79/89 [00:10<00:01,  7.57it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▋    | 80/89 [00:10<00:01,  7.57it/s]\u001b[A\n",
      " 91%|███████████████████████████████████████▏   | 81/89 [00:10<00:01,  7.57it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 82/89 [00:10<00:00,  7.43it/s]\u001b[A\n",
      " 93%|████████████████████████████████████████   | 83/89 [00:10<00:00,  7.40it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▌  | 84/89 [00:11<00:00,  7.44it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████  | 85/89 [00:11<00:00,  7.47it/s]\u001b[A\n",
      " 97%|█████████████████████████████████████████▌ | 86/89 [00:11<00:00,  7.50it/s]\u001b[A\n",
      " 98%|██████████████████████████████████████████ | 87/89 [00:11<00:00,  7.51it/s]\u001b[A\n",
      " 99%|██████████████████████████████████████████▌| 88/89 [00:11<00:00,  7.54it/s]\u001b[A\n",
      "100%|███████████████████████████████████████████| 89/89 [00:11<00:00,  7.55it/s]\u001b[A10/09/2022 21:19:35 - INFO - utils_qa -   Post-processing 702 example predictions split into 708 features.\n",
      "\n",
      "\n",
      "  0%|                                                   | 0/702 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "  4%|█▊                                       | 31/702 [00:00<00:03, 197.84it/s]\u001b[A\u001b[A\n",
      "\n",
      " 10%|███▉                                     | 67/702 [00:00<00:02, 274.83it/s]\u001b[A\u001b[A\n",
      "\n",
      " 17%|██████▊                                 | 120/702 [00:00<00:01, 377.53it/s]\u001b[A\u001b[A\n",
      "\n",
      " 25%|█████████▊                              | 173/702 [00:00<00:01, 432.27it/s]\u001b[A\u001b[A\n",
      "\n",
      " 32%|████████████▉                           | 226/702 [00:00<00:01, 464.97it/s]\u001b[A\u001b[A\n",
      "\n",
      " 40%|███████████████▊                        | 278/702 [00:00<00:00, 482.21it/s]\u001b[A\u001b[A\n",
      "\n",
      " 47%|██████████████████▊                     | 330/702 [00:00<00:00, 493.87it/s]\u001b[A\u001b[A\n",
      "\n",
      " 54%|█████████████████████▊                  | 382/702 [00:00<00:00, 499.95it/s]\u001b[A\u001b[A\n",
      "\n",
      " 62%|████████████████████████▋               | 433/702 [00:00<00:00, 500.36it/s]\u001b[A\u001b[A\n",
      "\n",
      " 69%|███████████████████████████▌            | 484/702 [00:01<00:00, 498.92it/s]\u001b[A\u001b[A\n",
      "\n",
      " 76%|██████████████████████████████▌         | 537/702 [00:01<00:00, 506.01it/s]\u001b[A\u001b[A\n",
      "\n",
      " 84%|█████████████████████████████████▌      | 588/702 [00:01<00:00, 504.33it/s]\u001b[A\u001b[A\n",
      "\n",
      " 91%|████████████████████████████████████▍   | 639/702 [00:01<00:00, 498.88it/s]\u001b[A\u001b[A\n",
      "\n",
      "100%|████████████████████████████████████████| 702/702 [00:01<00:00, 465.16it/s]\u001b[A\u001b[A\n",
      "10/09/2022 21:19:36 - INFO - utils_qa -   Saving predictions to ./run/predictions.json.\n",
      "10/09/2022 21:19:36 - INFO - utils_qa -   Saving nbest_preds to ./run/nbest_predictions.json.\n",
      "                                                                                \n",
      "\u001b[A{'exact_match': 36.03988603988604, 'f1': 71.0052124552993, 'epoch': 5.0}     \n",
      "100%|███████████████████████████████████| 30850/30850 [4:02:24<00:00,  2.55it/s]\n",
      "100%|███████████████████████████████████████████| 89/89 [00:14<00:00,  7.55it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:1850] 2022-10-09 21:19:37,044 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 14544.4802, 'train_samples_per_second': 16.968, 'train_steps_per_second': 2.121, 'train_loss': 1.423729950396323, 'epoch': 5.0}\n",
      "100%|███████████████████████████████████| 30850/30850 [4:02:24<00:00,  2.12it/s]\n",
      "[INFO|trainer.py:2640] 2022-10-09 21:19:37,047 >> Saving model checkpoint to ./run\n",
      "[INFO|configuration_utils.py:451] 2022-10-09 21:19:37,048 >> Configuration saved in ./run/config.json\n",
      "[INFO|modeling_utils.py:1566] 2022-10-09 21:19:37,627 >> Model weights saved in ./run/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2145] 2022-10-09 21:19:37,627 >> tokenizer config file saved in ./run/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2152] 2022-10-09 21:19:37,627 >> Special tokens file saved in ./run/special_tokens_map.json\n",
      "***** train metrics *****\n",
      "  epoch                    =        5.0\n",
      "  train_loss               =     1.4237\n",
      "  train_runtime            = 4:02:24.48\n",
      "  train_samples            =      49357\n",
      "  train_samples_per_second =     16.968\n",
      "  train_steps_per_second   =      2.121\n",
      "10/09/2022 21:19:37 - INFO - __main__ -   *** Evaluate ***\n",
      "[INFO|trainer.py:723] 2022-10-09 21:19:37,695 >> The following columns in the evaluation set don't have a corresponding argument in `ElectraForQuestionAnswering.forward` and have been ignored: offset_mapping, example_id. If offset_mapping, example_id are not expected by `ElectraForQuestionAnswering.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:3452] 2022-10-09 21:19:37,696 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:3453] 2022-10-09 21:19:37,696 >>   Num examples = 708\n",
      "[INFO|trainer.py:3454] 2022-10-09 21:19:37,696 >>   Batch size = 8\n",
      "100%|███████████████████████████████████████████| 89/89 [00:11<00:00,  7.60it/s]10/09/2022 21:19:50 - INFO - utils_qa -   Post-processing 702 example predictions split into 708 features.\n",
      "\n",
      "  0%|                                                   | 0/702 [00:00<?, ?it/s]\u001b[A\n",
      "  7%|███                                      | 52/702 [00:00<00:01, 519.64it/s]\u001b[A\n",
      " 15%|█████▉                                  | 105/702 [00:00<00:01, 520.70it/s]\u001b[A\n",
      " 23%|█████████                               | 158/702 [00:00<00:01, 520.65it/s]\u001b[A\n",
      " 30%|████████████                            | 211/702 [00:00<00:00, 523.75it/s]\u001b[A\n",
      " 38%|███████████████                         | 264/702 [00:00<00:00, 519.62it/s]\u001b[A\n",
      " 45%|██████████████████                      | 316/702 [00:00<00:00, 517.18it/s]\u001b[A\n",
      " 53%|█████████████████████                   | 369/702 [00:00<00:00, 519.74it/s]\u001b[A\n",
      " 60%|███████████████████████▉                | 421/702 [00:00<00:00, 513.50it/s]\u001b[A\n",
      " 67%|██████████████████████████▉             | 473/702 [00:00<00:00, 504.88it/s]\u001b[A\n",
      " 75%|█████████████████████████████▉          | 526/702 [00:01<00:00, 510.35it/s]\u001b[A\n",
      " 82%|████████████████████████████████▉       | 578/702 [00:01<00:00, 512.91it/s]\u001b[A\n",
      " 90%|███████████████████████████████████▉    | 630/702 [00:01<00:00, 507.10it/s]\u001b[A\n",
      "100%|████████████████████████████████████████| 702/702 [00:01<00:00, 508.95it/s]\u001b[A\n",
      "10/09/2022 21:19:51 - INFO - utils_qa -   Saving predictions to ./run/predictions.json.\n",
      "10/09/2022 21:19:51 - INFO - utils_qa -   Saving nbest_preds to ./run/nbest_predictions.json.\n",
      "100%|███████████████████████████████████████████| 89/89 [00:14<00:00,  6.33it/s]\n",
      "***** eval metrics *****\n",
      "  epoch        =     5.0\n",
      "  eval_samples =     708\n",
      "  exact_match  = 36.0399\n",
      "  f1           = 71.0052\n"
     ]
    }
   ],
   "source": [
    "!python src/finetuningQA/run_qa.py \\\n",
    "  --model_name_or_path $model_path \\\n",
    "  --dataset_name src/finetuningQA/custom_dataset_loader.py \\\n",
    "  --do_train \\\n",
    "  --do_eval \\\n",
    "  --per_device_train_batch_size 8 \\\n",
    "  --gradient_accumulation_steps 1 \\\n",
    "  --learning_rate 3e-5 \\\n",
    "  --num_train_epochs 5 \\\n",
    "  --max_seq_length 384 \\\n",
    "  --doc_stride 128 \\\n",
    "  --output_dir ./run \\\n",
    "  --n_best_size 20 \\\n",
    "  --evaluation_strategy epoch \\\n",
    "  --save_steps 10000 \\\n",
    "  --seed 666 \\\n",
    "  --overwrite_output_dir \\\n",
    "  --warmup_steps 500"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2501290",
   "metadata": {},
   "source": [
    "## Move the new pretrained model to the \"arabertv2-QA-model/\" to be able to run inference with it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "51ee359f",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.replace(\"run/pytorch_model.bin\",\"araelectra-QA-model/pytorch_model.bin\")\n",
    "os.replace(\"run/tokenizer.json\",\"araelectra-QA-model/tokenizer.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "debd847e",
   "metadata": {},
   "source": [
    "## Run Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "507205f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_1 = (\n",
    "  'يعد ملف الإسكان بمملكة البحرين من أبرز الملفات المعيشة'\n",
    "  'التي تحظى بالأولوية لدى المواطن البحريني، وهو ما وعت إليه'\n",
    "  'القيادة الرشيدة منذ عقود طويلة، وتحديداً منذ مطلع ستينيات القرن الماضي، عندما تم تأسيس لجنة'\n",
    "  'الإسكان والتمليك عام 1962، والتي وضعت التشريع الأول المنظم لسياسة تقديم الخدمات'\n",
    "  'الإسكانية للمواطنين، ليعقب ذلك بعام واحد إنشاء مشروع مدينة عيسى كأول مدينة'\n",
    "  'إسكانية متكاملة في المملكة تلبي احتياجات المواطنين في الحصول على السكن.'\n",
    "  \n",
    "  )\n",
    "\n",
    "question1 = \"متى تم تأسيس لجنة الإسكان والتمليك\"\n",
    "question2 = \"ما هي أول مدينة اسكانية تم تأسيسيها في البحرين\"\n",
    "\n",
    "\n",
    "context_2 = (\n",
    "'المكتبة البرلمانية هي مكتبة متخصصة ومركز للمعلومات والدراسات والبحوث القانونية تتبع إدارة البحوث والدراسات بالأمانة  العامة المساعدة لشؤون العلاقات' \n",
    "'والإعلام والبحوث بمجلس الشورى، وقد أنشئت عام 2016م لخدمة أعضاء المجلس، بالإضافة إلى فريق العمل الخاص بالأعضاء والعاملين'\n",
    "'بالمجلس والباحثين في المجال نفسه من خارج المجلس كي تمدهم بالمعلومات اللازمة، كما تؤدي دورًاً أرشيفيًاً لحفظ الوثائق الخاصة'\n",
    "'بأعمال البرلمان وجلساته،  وهي عبارة  عن  قاعة كبيرة تضم زهاء 4000 عنوان من الكتب العامة والمتخصصة والمراجع'\n",
    "'والدوريات المتخصصة والمصادر الإلكترونية، والبحوث. .الهيكل التنظيمي للمكتبة. ترتبط المكتبة البرلمانية'\n",
    "'هيكليا بإدارة البحوث والدراسات التي تتبع الأمانة العامة المساعدة لشؤون العلاقات والإعلام والبحوث'\n",
    ")\n",
    "\n",
    "question3 = \"ما هي المكتبة البرلمانية\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3815c184",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction {'score': 0.7516324520111084, 'start': 22, 'end': 78, 'answer': 'مكتبة متخصصة ومركز للمعلومات والدراسات والبحوث القانونية'}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering, pipeline\n",
    "from src.finetuningQA.preprocess import ArabertPreprocessor\n",
    "\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(\"araelectra-QA-model/\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"araelectra-QA-model/\")\n",
    "\n",
    "arabert_prep = ArabertPreprocessor(model_name=model_name)\n",
    "\n",
    "context_ = arabert_prep.preprocess(context_2)\n",
    "question_ = arabert_prep.preprocess(question3)\n",
    "\n",
    "qa_model = pipeline(\"question-answering\", model=model, tokenizer=tokenizer)\n",
    "print(\"prediction {}\".format(qa_model(question = question_, context = context_)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20cd84f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e21cd62c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('python37')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "368899a92c96b77b8737d0c0899584e6db5764acf4a0aebf08a9f2dc81b165b5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
