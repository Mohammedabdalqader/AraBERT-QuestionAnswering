{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1172cfc2",
   "metadata": {},
   "source": [
    "## Fine-tune AraBERTv2 on Custom Dataset\n",
    "\n",
    "This is a POC for Fine-tuning AraBERTv2 or other available pre-trained models: https://huggingface.co/aubmindlab on a Custom Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4dd748",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.finetuningQA.data_split import train_dev_test_split\n",
    "import json\n",
    "import datasets\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a475d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# arabert2 model local path. \n",
    "# You can download another pretrained models from here: https://huggingface.co/aubmindlab\n",
    "model_path= \"bert-base-arabertv2/\"\n",
    "model_name = \"aubmindlab/bert-base-arabertv2\"\n",
    "\n",
    "!wget https://huggingface.co/aubmindlab/bert-base-arabertv2/resolve/main/pytorch_model.bin -O bert-base-arabertv2/pytorch_model.bin\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e337eeac",
   "metadata": {},
   "source": [
    "## Splitt the dataset into train/dev/test (if needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66b3ce5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_dev_test_split(\"dataset/question-answering-dataset/iskan_train.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d1e5d0",
   "metadata": {},
   "source": [
    "## Pre-process the data\n",
    "Here we will prepare the dataset in the format that the model expect.\n",
    "\n",
    "The Dataset structure must be as follow:\n",
    "\n",
    "'''\n",
    "\n",
    "    {\"data\": \n",
    "\n",
    "        [ \n",
    "            {\"paragraphs\": \n",
    "\n",
    "                [\n",
    "                    {\"qas\": \n",
    "            \n",
    "                    [{\"question\": \"ما هو برنامج مزايا\", \"id\": 445985, \n",
    "\n",
    "                        \"answers\": [\"text\": \"هو أحد المبادرات الجديدة التي تطرحها وزارة الإسكان والتخطيط العمراني بالتعاون مع القطاع الخاص لتوفير السكن الاجتماعي للمواطنين المدرجة أسمائهم على قوائم الانتظار\"}, \n",
    "            \n",
    "                    {\"question\": \"ما هو اقصى حد لقيمة القسط الشهري المترتب على المواطن؟\", \"id\": 445986, \n",
    "                \n",
    "                        \"answers\": [\"text\": \"25% من راتب المواطن كحد أقصى.\"]\n",
    "\n",
    "                    \"context\": \"نبذة عن برنامج \"مزايا \" برنامج \"مزايا \" هو أحد المبادرات الجديدة التي تطرحها وزارة الإسكان والتخطيط العمراني بالتعاون مع القطاع الخاص لتوفير السكن الاجتماعي للمواطنين المدرجة أسمائهم على قوائم الانتظار، وتقوم فكرة البرنامج على قيام المواطن الذي تنطبق عليه المعايير بشراء وحدة سكنية من خلال حصوله على تمويل من أحد البنوك المشاركة، على أن تقوم الحكومة بتوفير الدعم المالي للمواطن، والمتمثل في سداد الفارق بين قيمة القسط الفعلي لمبلغ التمويل المحدد من قبل البنك الممول، وقيمة القسط الشهري المستحق على المواطن والذي لا يتجاوز 25% من راتب المواطن كحد أقصى. \",\n",
    "            \"document_id\": 862498\"}\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "\n",
    "    }\n",
    "\n",
    "        \n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e14ef2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Dataset\n",
    "!python src/finetuningQA/preprocessing.py \\\n",
    "  --input_file=\"dataset/question-answering-dataset/iskan_new.json\" \\\n",
    "  --output_file=\"dataset/question-answering-dataset/iskan_new_preprocessed.json\" \\\n",
    "  --model_name=$model_name\n",
    "\n",
    "\n",
    "# Eval Dataset\n",
    "'''\n",
    "!python src/finetuningQA/preprocessing.py \\\n",
    "  --input_file=\"dataset/question-answering-dataset/iskan_dev.json\" \\\n",
    "  --output_file=\"dataset/question-answering-dataset/iskan_dev_preprocessed.json\" \\\n",
    "  --model_name=$model_name\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "15d3ab47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'paragraphs': [{'qas': [{'question': 'متى تم تأسيس لجن +ة ال+ إسكان و+ ال+ تمليك', 'id': 452838, 'answers': [{'answer_id': 559363, 'document_id': 976441, 'question_id': 452838, 'text': 'عام 1962', 'answer_start': 285, 'answer_end': 231, 'answer_category': None}], 'is_impossible': False}, {'question': 'ما هي ال+ مدين +ة ال+ أولى التي تم إنشائ +ها ل+ تلبي +ة حاج +ات ال+ مواطن +ين', 'id': 452839, 'answers': [{'answer_id': 559364, 'document_id': 976441, 'question_id': 452839, 'text': 'مدين +ة عيسى', 'answer_start': 436, 'answer_end': 350, 'answer_category': None}], 'is_impossible': False}], 'context': 'يعد ملف ال+ إسكان ب+ مملك +ة ال+ بحرين من أبرز ال+ ملف +ات ال+ معيشي +ة التي تحظى ب+ ال+ أولوي +ة لدى ال+ مواطن ال+ بحريني ، و+ هو ما وع +ت إلي +ه ال+ قياد +ة ال+ رشيد +ة منذ عقود طويل +ة ، و+ تحديد +ا منذ مطلع ستيني +ات ال+ قرن ال+ ماضي ، عندما تم تأسيس لجن +ة ال+ إسكان و+ ال+ تمليك عام 1962 ، و+ التي وضع +ت ال+ تشريع ال+ أول ال+ منظم ل+ سياس +ة تقديم ال+ خدم +ات ال+ إسكاني +ة ل+ ال+ مواطن +ين ، ل+ يعقب ذلك ب+ عام واحد إنشاء مشروع مدين +ة عيسى ك+ أول مدين +ة إسكاني +ة متكامل +ة في ال+ مملك +ة تلبي احتياج +ات ال+ مواطن +ين في ال+ حصول على ال+ سكن .', 'document_id': 976441}]}\n"
     ]
    }
   ],
   "source": [
    "# Load the preprocessed dataset and print one sample to see how the preprocessed data looks like\n",
    "with open(\"dataset/question-answering-dataset/iskan_dev_preprocessed.json\") as f:\n",
    " data = json.load(f)['data']\n",
    "\n",
    "print(data[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4155cce",
   "metadata": {},
   "source": [
    "## Custom Dataset Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "240d2b6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/finetuningQA/custom_dataset_loader.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/finetuningQA/custom_dataset_loader.py\n",
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import json\n",
    "import logging\n",
    "\n",
    "import datasets\n",
    "\n",
    "\n",
    "_CITATION = \"\"\"\\\n",
    "@inproceedings{mozannar-etal-2019-neural,\n",
    "    title = {Neural {A}rabic Question Answering},\n",
    "    author = {Mozannar, Hussein  and Maamary, Elie  and El Hajal, Karl  and Hajj, Hazem},\n",
    "    booktitle = {Proceedings of the Fourth Arabic Natural Language Processing Workshop},\n",
    "    month = {aug},\n",
    "    year = {2019},\n",
    "    address = {Florence, Italy},\n",
    "    publisher = {Association for Computational Linguistics},\n",
    "    url = {https://www.aclweb.org/anthology/W19-4612},\n",
    "    doi = {10.18653/v1/W19-4612},\n",
    "    pages = {108--118},\n",
    "    abstract = {This paper tackles the problem of open domain factual Arabic question answering (QA) using Wikipedia as our knowledge source. This constrains the answer of any question to be a span of text in Wikipedia. Open domain QA for Arabic entails three challenges: annotated QA datasets in Arabic, large scale efficient information retrieval and machine reading comprehension. To deal with the lack of Arabic QA datasets we present the Arabic Reading Comprehension Dataset (ARCD) composed of 1,395 questions posed by crowdworkers on Wikipedia articles, and a machine translation of the Stanford Question Answering Dataset (Arabic-SQuAD). Our system for open domain question answering in Arabic (SOQAL) is based on two components: (1) a document retriever using a hierarchical TF-IDF approach and (2) a neural reading comprehension model using the pre-trained bi-directional transformer BERT. Our experiments on ARCD indicate the effectiveness of our approach with our BERT-based reader achieving a 61.3 F1 score, and our open domain system SOQAL achieving a 27.6 F1 score.}\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "_DESCRIPTION = \"\"\"\\\n",
    " Custom Dataset Loader \n",
    "\"\"\"\n",
    "\n",
    "_URL = \"../../dataset/question-answering-dataset/\"\n",
    "_URLs = {\n",
    "    \"train\": _URL + \"iskan_new_preprocessed.json\",\n",
    "    \"dev\": _URL + \"iskan_new_preprocessed.json\",\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "class CustomDatasetAraBERTConfig(datasets.BuilderConfig):\n",
    "    \"\"\"BuilderConfig for Custom Dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        \"\"\"BuilderConfig for CustomDataset.\n",
    "\n",
    "        Args:\n",
    "          **kwargs: keyword arguments forwarded to super.\n",
    "        \"\"\"\n",
    "        super(CustomDatasetAraBERTConfig, self).__init__(**kwargs)\n",
    "\n",
    "\n",
    "class CustomDatasetAraBERT(datasets.GeneratorBasedBuilder):\n",
    "    \"\"\"CustomDataset\"\"\"\n",
    "\n",
    "    BUILDER_CONFIGS = [\n",
    "        CustomDatasetAraBERTConfig(\n",
    "            name=\"plain_text\",\n",
    "            version=datasets.Version(\"1.0.0\", \"\"),\n",
    "            description=\"Plain text\",\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    def _info(self):\n",
    "        return datasets.DatasetInfo(\n",
    "            description=_DESCRIPTION,\n",
    "            features=datasets.Features(\n",
    "                {\n",
    "                    \"id\": datasets.Value(\"string\"),\n",
    "                    \"title\": datasets.Value(\"string\"),\n",
    "                    \"context\": datasets.Value(\"string\"),\n",
    "                    \"question\": datasets.Value(\"string\"),\n",
    "                    \"answers\": datasets.features.Sequence(\n",
    "                        {\"text\": datasets.Value(\"string\"), \"answer_start\": datasets.Value(\"int32\")}\n",
    "                    ),\n",
    "                }\n",
    "            ),\n",
    "            # No default supervised_keys (as we have to pass both question\n",
    "            # and context as input).\n",
    "            supervised_keys=None,\n",
    "            citation=_CITATION,\n",
    "        )\n",
    "\n",
    "    def _split_generators(self, dl_manager):\n",
    "        urls_to_download = _URLs\n",
    "        downloaded_files = dl_manager.download_and_extract(urls_to_download)\n",
    "\n",
    "        return [\n",
    "            datasets.SplitGenerator(name=datasets.Split.TRAIN, gen_kwargs={\"filepath\": downloaded_files[\"train\"]}),\n",
    "            datasets.SplitGenerator(name=datasets.Split.VALIDATION, gen_kwargs={\"filepath\": downloaded_files[\"dev\"]}),\n",
    "        ]\n",
    "\n",
    "    def _generate_examples(self, filepath):\n",
    "        \"\"\"This function returns the examples in the raw (text) form.\"\"\"\n",
    "        logging.info(\"generating examples from = %s\", filepath)\n",
    "        with open(filepath, encoding=\"utf-8\") as f:\n",
    "            data_ = json.load(f)\n",
    "            for article in data_[\"data\"]:\n",
    "                title = article.get(\"title\", \"\").strip()\n",
    "                for paragraph in article[\"paragraphs\"]:\n",
    "                    context = paragraph[\"context\"].strip()\n",
    "                    for qa in paragraph[\"qas\"]:\n",
    "                        question = qa[\"question\"].strip()\n",
    "\n",
    "                        id_ = qa[\"id\"]\n",
    "\n",
    "                        answer_starts = [answer[\"answer_start\"] for answer in qa[\"answers\"]]\n",
    "                        answers = [answer[\"text\"].strip() for answer in qa[\"answers\"]]                     \n",
    "\n",
    "                        # Features currently used are \"context\", \"question\", and \"answers\".\n",
    "                        # Others are extracted here for the ease of future expansions.\n",
    "                        yield id_, {\n",
    "                            \"title\": title,\n",
    "                            \"context\": context,\n",
    "                            \"question\": question,\n",
    "                            \"id\": id_,\n",
    "                            \"answers\": {\"answer_start\": answer_starts, \"text\": answers},\n",
    "                        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "99bd1d0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset custom_dataset_loader (/home/mohammed/.cache/huggingface/datasets/custom_dataset_loader/plain_text/1.0.0/cd8bb2354a58e1b2546ed6aade8861259cb6876a21e2f09793fede249feae5df)\n",
      "100%|██████████| 2/2 [00:00<00:00,  9.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
      "        num_rows: 28\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
      "        num_rows: 28\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "iskan = datasets.load_dataset(\"src/finetuningQA/custom_dataset_loader.py\")\n",
    "print(iskan)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "512064d7",
   "metadata": {},
   "source": [
    "## Fine-tuning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c181511",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python src/finetuningQA/run_qa.py \\\n",
    "  --model_name_or_path $model_path \\\n",
    "  --dataset_name src/finetuningQA/custom_dataset_loader.py \\\n",
    "  --do_train \\\n",
    "  --do_eval \\\n",
    "  --per_device_train_batch_size 2 \\\n",
    "  --gradient_accumulation_steps 1 \\\n",
    "  --learning_rate 3e-5 \\\n",
    "  --num_train_epochs 20 \\\n",
    "  --max_seq_length 384 \\\n",
    "  --doc_stride 128 \\\n",
    "  --output_dir ./run \\\n",
    "  --n_best_size 20 \\\n",
    "  --evaluation_strategy epoch \\\n",
    "  --save_steps 1000 \\\n",
    "  --seed 666 \\\n",
    "  --overwrite_output_dir \\\n",
    "  --warmup_steps 50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2501290",
   "metadata": {},
   "source": [
    "## Move the new pretrained model to the \"arabertv2-QA-model/\" to be able to run inference with it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "51ee359f",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.replace(\"run/pytorch_model.bin\",\"arabertv2-QA-model/pytorch_model.bin\")\n",
    "os.replace(\"run/tokenizer.json\",\"arabertv2-QA-model/tokenizer.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "debd847e",
   "metadata": {},
   "source": [
    "## Run Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "507205f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_1 = (\n",
    "  'يعد ملف الإسكان بمملكة البحرين من أبرز الملفات المعيشة'\n",
    "  'التي تحظى بالأولوية لدى المواطن البحريني، وهو ما وعت إليه'\n",
    "  'القيادة الرشيدة منذ عقود طويلة، وتحديداً منذ مطلع ستينيات القرن الماضي، عندما تم تأسيس لجنة'\n",
    "  'الإسكان والتمليك عام 1962، والتي وضعت التشريع الأول المنظم لسياسة تقديم الخدمات'\n",
    "  'الإسكانية للمواطنين، ليعقب ذلك بعام واحد إنشاء مشروع مدينة عيسى كأول مدينة'\n",
    "  'إسكانية متكاملة في المملكة تلبي احتياجات المواطنين في الحصول على السكن.'\n",
    "  \n",
    "  )\n",
    "\n",
    "question1 = \"متى تم تأسيس لجنة الإسكان والتمليك\"\n",
    "question2 = \"ما هي أول مدينة اسكانية تم تأسيسيها في البحرين\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3815c184",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-08-27 22:32:45,736 - farasapy_logger - WARNING]: Be careful with large lines as they may break on interactive mode. You may switch to Standalone mode for such cases.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "prediction {'score': 0.9743682742118835, 'start': 269, 'end': 277, 'answer': 'عام 1962'}\n",
      "The answer after unprocessing is : عام 1962\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering, pipeline\n",
    "from src.finetuningQA.preprocess import ArabertPreprocessor\n",
    "\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(\"arabertv2-QA-model/\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"arabertv2-QA-model/\")\n",
    "\n",
    "arabert_prep = ArabertPreprocessor(model_name=model_name)\n",
    "\n",
    "context_ = arabert_prep.preprocess(context_1)\n",
    "question_ = arabert_prep.preprocess(question1)\n",
    "\n",
    "qa_model = pipeline(\"question-answering\", model=model, tokenizer=tokenizer)\n",
    "print(\"prediction {}\".format(qa_model(question = question_, context = context_)))\n",
    "\n",
    "print(\"The answer after unprocessing is : {}\".format(arabert_prep.unpreprocess(qa_model(question = question_, context = context_)[\"answer\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20cd84f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e21cd62c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('python37')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "368899a92c96b77b8737d0c0899584e6db5764acf4a0aebf08a9f2dc81b165b5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
